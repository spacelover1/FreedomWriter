<!DOCTYPE html>
<html dir="rtl" lang="fa">
  
  <head><meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta name="author" content="نویسنده" />
<meta name="copyright" content="Commons Attribution 4.0 International" />
<meta name="robot" content="" />
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="description" content="" />

<link rel="canonical" href="https://spacelover.ir/hello-world.html" />
<link rel="icon" href="" />
<link rel="stylesheet" href="https://spacelover.ir/assets/css/main.css" />
<link rel="stylesheet" href="https://spacelover.ir/assets/fonts/fontawesomev5.0.2.css" />
<meta name="keywords" content='nlp,اموزش' /><title>مبانی پردازش زبان طبیعی(NLP)- سه - نویسنده آزاد</title><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>مبانی پردازش زبان طبیعی(NLP)- سه | نویسنده آزاد</title>
<meta name="generator" content="Jekyll v4.3.1" />
<meta property="og:title" content="مبانی پردازش زبان طبیعی(NLP)- سه" />
<meta name="author" content="نویسنده" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="در این قسمت در مورد برداری کردن دیتا صحبت می کنیم. تا اینجا دیتا رو خوندیم و تا حدودی نرمالیزه کردیم. الان پایتون دیتا رو فقط یک سری رشته کاراکتر می بینه. حالا برای اینکه مدل ماشین لرنینگ و پایتون این دیتا رو درک کنه باید دیتا برداری بشه. برداری کردن یعنی چی؟ یعنی متن به عددصحیح تبدیل شه و یک بردار ویژگی ساخته شه. حالا بردار ویژگی در اینجا یعنی متن هر پیام رو بگیریم و به یک بردارعددی تبدیل کنیم که نمایش دهنده متن اون پیام باشه. چطوری این کار رو انجام می دیم؟ در ادامه درباره این مورد صحبت می کنیم. چندین روش برای برداری کردن ویژگی ها وجود داره که در ادامه سه روش رایج رو بررسی می کنیم." />
<meta property="og:description" content="در این قسمت در مورد برداری کردن دیتا صحبت می کنیم. تا اینجا دیتا رو خوندیم و تا حدودی نرمالیزه کردیم. الان پایتون دیتا رو فقط یک سری رشته کاراکتر می بینه. حالا برای اینکه مدل ماشین لرنینگ و پایتون این دیتا رو درک کنه باید دیتا برداری بشه. برداری کردن یعنی چی؟ یعنی متن به عددصحیح تبدیل شه و یک بردار ویژگی ساخته شه. حالا بردار ویژگی در اینجا یعنی متن هر پیام رو بگیریم و به یک بردارعددی تبدیل کنیم که نمایش دهنده متن اون پیام باشه. چطوری این کار رو انجام می دیم؟ در ادامه درباره این مورد صحبت می کنیم. چندین روش برای برداری کردن ویژگی ها وجود داره که در ادامه سه روش رایج رو بررسی می کنیم." />
<link rel="canonical" href="https://spacelover.ir/nlp-basics-three.html" />
<meta property="og:url" content="https://spacelover.ir/nlp-basics-three.html" />
<meta property="og:site_name" content="نویسنده آزاد" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-07-15T00:00:00+04:30" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="مبانی پردازش زبان طبیعی(NLP)- سه" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"نویسنده"},"dateModified":"2021-07-15T00:00:00+04:30","datePublished":"2021-07-15T00:00:00+04:30","description":"در این قسمت در مورد برداری کردن دیتا صحبت می کنیم. تا اینجا دیتا رو خوندیم و تا حدودی نرمالیزه کردیم. الان پایتون دیتا رو فقط یک سری رشته کاراکتر می بینه. حالا برای اینکه مدل ماشین لرنینگ و پایتون این دیتا رو درک کنه باید دیتا برداری بشه. برداری کردن یعنی چی؟ یعنی متن به عددصحیح تبدیل شه و یک بردار ویژگی ساخته شه. حالا بردار ویژگی در اینجا یعنی متن هر پیام رو بگیریم و به یک بردارعددی تبدیل کنیم که نمایش دهنده متن اون پیام باشه. چطوری این کار رو انجام می دیم؟ در ادامه درباره این مورد صحبت می کنیم. چندین روش برای برداری کردن ویژگی ها وجود داره که در ادامه سه روش رایج رو بررسی می کنیم.","headline":"مبانی پردازش زبان طبیعی(NLP)- سه","mainEntityOfPage":{"@type":"WebPage","@id":"https://spacelover.ir/nlp-basics-three.html"},"url":"https://spacelover.ir/nlp-basics-three.html"}</script>
<!-- End Jekyll SEO tag -->


  </head>

  <body><header>
  <div class="wrapper">
    <a class="site-title" href="https://spacelover.ir/">نویسنده آزاد</a>
    <small id="motto"> نوشته‌های یک دختر ایرانی </small>
    <nav>
      <a href="/test">تست برگه</a>
    </nav>
  </div>
</header><main>
      <div class="wrapper">

<article class="post-content">

  <div class="post-header">
    <h1 class="post-title">مبانی پردازش زبان طبیعی(NLP)- سه</h1>
    <p>
      <i class="fas fa-calendar"></i>
      پنج‌شنبه ۲۴ تیر ۱۴۰۰<br>
      
<!--       <i class="fas fa-stopwatch"></i>
      <p>در این قسمت در مورد برداری کردن دیتا صحبت می کنیم. <br />
تا اینجا دیتا رو خوندیم و تا حدودی نرمالیزه کردیم. الان پایتون دیتا رو فقط یک سری رشته کاراکتر می بینه. حالا برای اینکه مدل ماشین لرنینگ و پایتون این دیتا رو درک کنه باید دیتا برداری بشه. برداری کردن یعنی چی؟ یعنی متن به عددصحیح تبدیل شه و یک بردار ویژگی ساخته شه.<br />
حالا بردار ویژگی در اینجا یعنی متن هر پیام رو بگیریم و به یک بردارعددی تبدیل کنیم که نمایش دهنده متن اون پیام باشه. <br />
چطوری این کار رو انجام می دیم؟ در ادامه درباره این مورد صحبت می کنیم.<br />
چندین روش برای برداری کردن ویژگی ها وجود داره که در ادامه سه روش رایج رو بررسی می کنیم.</p>

<h۲ id="روش-اول-بردار-تعداد-count-vectorization"><strong>روش اول: بردار تعداد (Count Vectorization)</strong></h۲>

<p>در این روش هر پیام گرفته می شه و هر کلمه به عنوان یک ویژگی در نظر گرفته می شه و بعد تعداد تکرار هر کلمه در اون پیام ثبت می شه. در نهایت یک ماتریسی داریم که هر سطر مربوط به یک پیام و هر ستون نمایش دهنده یک کلمه است. و در نهایت پایتون با بررسی این ماتریس یک ارتباطی بین کلمات موجود در پیام و لیبل اون پیام پیدا می کنه تا در آینده که بهش پیام های بدون لیبل بدیم بتونه به درسی برچسب گذاری کنه.</p>

<p>برای درک بهتر این فرایند به عکس زیر دقت کنید:</p>

<div style="text-align:center"><img src="https://raw.githubusercontent.com/spacelover۱/NLP-with-Python/main/۳-VectorizingRawData/vectorization_example.PNG" alt="vectorization_example" /></div>

<p>دراین تصویر فقط دو رشته offer و lol از لیست کلمات پیام ها انتخاب شده و تعداد تکرارشون محاسبه شده. همونطور که در جدول سمت چپ و راست می بینید پیام هایی که برچسب غیر اسپم دارند در آن ها رشته lol وجود داشته و تکرار شده ولی شامل رشته offer نیستند و برعکس پیام های اسپم اکثرا رشته offer رو شامل می شن. <br />
این یک مثال بسیار ساده برای درک فرایند و مفهوم بردار تعداد است.</p>

<p>حالا در عمل این روش رو پیاده می کنیم:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import pandas as pd
import re
import string
import nltk

pd.set_option('display.max_colwidth', ۱۰۰)
dataset = pd.read_csv('SMSSpamCollection.tsv', sep='\t')
dataset.columns = ['label', 'body']

nltk.download('stopwords')
stopwords = nltk.corpus.stopwords.words('english')
ps = nltk.PorterStemmer()


def clean_text(text):
  text = "".join([word.lower() for word in text if word not in string.punctuation])
  tokens = re.split('\W+', text)
  text = [ps.stem(word) for word in tokens if word not in stopwords]
  return text
</code></pre></div></div>

<p>بعد از خوندن و پاکسازی دیتا، سراغ برداری کردن می ریم.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from sklearn.feature_extraction import CountVectorizer

count_vect = CountVectorizer(analyzer=func_name)
X_counts = count_vect.fit_transform(dataset['body'])
</code></pre></div></div>

<p>حالا می تونیم با استفاده از <code class="language-plaintext highlighter-rouge">X_counts.shape</code> تعداد پیام ها و تعداد رشته های منحصر بفرد در این پیام ها رو ببینیم. در این دیتاست ۵۵۶۷ پیام و ۸۱۰۴ رشته منحصر بفرد داریم که همون ویژگی های ما هستند. این اعداد تعداد سطرو و ستون های ماتریس رو نمایش می ده.<br />
و <code class="language-plaintext highlighter-rouge">count_vect.get_feature_names()</code> رشته های منحصربفرد رو نمایش می ده.</p>

<p>تابع هایپرپارامترهای دیگه ای هم داره که <a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html">اینجا</a> می تونید دربارشون بخونید.</p>

<p>حالا در اینجا برای یادگیری ۲۰ پیام اول رو  برداری می کنیم:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sample = dataset[۰:۲۰]
count_vect_sample = CountVectorizer(analyzer=clean_text)
X_counts_sample = count_vect_sample.fit_transform(dataset['body'])
</code></pre></div></div>

<p>و الان وقتی سایز دیتای نمونه رو ببینیم ۱۹۲ رشته منحصربفرد داریم. خروجی و کد کامل این بخش رو <a href="https://github.com/spacelover۱/NLP-with-Python/blob/main/۳-VectorizingRawData/CountVectorization.ipynb">اینجا</a> می تونید ببینید.</p>

<h۲ id="روش-دوم-بردار-n-grams-n-gram-vectorizing"><strong>روش دوم: بردار N-Grams (N-gram vectorizing)</strong></h۲>

<p>این روش هم تا حدود زیادی مشابه روش قبلیه و ساختار کدش مشابه اونه. در اینجا هم هر سطر پیام ها هستند ولی هر ستون به جای نمایش یک رشته، ترکیب nتایی از رشته هاست. برای درک بهتر تصویر زیر  رو ببینید:</p>

<div style="text-align:center"><img src="https://raw.githubusercontent.com/spacelover۱/NLP-with-Python/main/۳-VectorizingRawData/ngrams.png?token=AEGZAVTZYIIT۲UNASADUQN۳A۶KJCE" alt="ngrams" /></div>

<p>مثل مرحله قبل دیتا رو می خونیم و بعد باید تابعی بنویسیم که مراحل پاکسازی رو انجام بده. بخش قبل یک لیستی از توکن رو می دادیم به vectorizer اما الان چون می خواد ترکیبی از کلمه ها رو بسازه باید ورودی بهش یک رشته بدیم. پس در آخر باید توکن ها رو مثل یک جمله کنار هم دیگه قرار بدیم و این کار رو با تابع <code class="language-plaintext highlighter-rouge">join()</code> انجام می دیم:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def clean_text(text):
  text = "".join([word.lower() for word in text if word not in string.punctuation])
  tokens = re.split('\W+', text)
  text = " ".join([ps.stem(word) for word in tokens if word not in stopwords])
  return text
</code></pre></div></div>

<p>اگر یادتون باشه خط اول کاراکترها رو یکی یکی بررسی می کرد برای پیدا کردن علائم نگارشی و در اخر با <code class="language-plaintext highlighter-rouge">join()</code> این کاراکترها رو بهم متصل کردیم. در <code class="language-plaintext highlighter-rouge">join()</code> دومی قرار کلمه ها کنار هم بیان تا جمله بسازن پس باید یک فاصله بین هر کلمه باشه.</p>

<p>در اینجا هم از CountVectorizer استفاده می کنیم:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ngram_vect = CountVectorizer(ngram_range=(۲,۲))
X_counts = ngram_vect.fit_transform(dataset['cleaned_text'])
</code></pre></div></div>

<p>در <code class="language-plaintext highlighter-rouge">ngram_range</code> مشخص می کنیم که ترکیب چندتایی از کلمه ها بسازه. مثلا (۱,۳) یعنی همه ترکیبهای یکی، دوتایی و سه تایی. <br />
همونطور که مشاهده می کنید تعداد فیچرها اینجا خیلی زیاد می شه. نکته ای که باید توجه کنیم اینه که چه زمانی از هر کدوم از این روش ها استفاده کنیم. با توجه به مسئله ممکنه یکی از این روش ها نتیجه بهتری بده. لزوما نمی شه گفت یکی از این روش ها بهتر از دیگری است.</p>

<p>کدهای این بخش در <a href="https://github.com/spacelover۱/NLP-with-Python/blob/main/۳-VectorizingRawData/NGrams.ipynb">اینجا</a> قابل مشاهده است.</p>

<h۲ id="روش-سوم-te-idf-term-frequency--inverse-document-frequenct"><strong>روش سوم: TE-IDF (Term Frequency- Inverse Document Frequenct)</strong></h۲>

<p>در این روش هم یک ماتریس ایجاد می شه که سطرها پیام ها هستند و هر ستون یک کلمه رو مشخص می کنه. اما سلول های این ماتریس دیگه تعداد تکرار کلمه رو نشون نمی ده بلکه وزن اون کلمه رو نشون می ده، تا اهمیت هر کلمه رو در اون پیام مشخص کنه. <br />
فرمول زیر برای محاسبه این وزنه:</p>

<div style="text-align:center"><img src="https://raw.githubusercontent.com/spacelover۱/NLP-with-Python/main/۳-VectorizingRawData/tf-idf.PNG?token=AEGZAVSASZLPTSDT۷۵HLYL۳A۶PNMY" alt="tf-idf_formula" /></div>

<p>بریم ببینیم هر کدوم از این عبارات در فرمول چیو مشخص می کنه و چطوری محاسبه می شه:</p>

<p>عبارت tf تعداد تکرار یک کلمه در یک جمله تقسیم بر تعداد کل کلمات اون جمله. <br />
مثلا در جمله “امروز هوا گرم است” اگر کلمه “گرم” رو در نظر بگیریم، مقدار tf می شه: ۱/۴ یا ۰.۲۵</p>

<p>قسمت دوم این فرمول مشخص می کنه که هر کلمه چند بار تو کل جملات (پیام ها) تکرار شده.
در همین مثال اگر متن ما شامل ۲۰ جمله باشه و کلمه گرم فقط یک بار تکرار شده باشه، نتیجه می شه:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>N = ۲۰, df = ۱ &gt;&gt;&gt;&gt; log(N/df) = log(۲۰/۱) = ۱.۳۰۱
</code></pre></div></div>

<p>مبنای لگاریتم هم ۱۰ است.</p>

<p>و درنهایت:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>۰.۲۵ * ۱.۳۰۱ = ۰.۳۲۵
</code></pre></div></div>

<p>هر چقدر مقدار داخل لگاریتم بزرگتر باشه، لگاریتم اون مقدارم بزرگتر می شه. مثلا فرض کنید تعداد کل جملات ۴۰ باشه مقدار لگاریتم می شه ۱.۶ یعنی بیشتر از مقدار قبل. پس طبق این فرمول هر چقدر یک کلمه در متن کمتر تکرار شده باشه، عددی که تولید می شه بزرگتره. <br />
و اگر یک کلمه در یک جمله خیلی تکرار شده باشه ولی در کل متن خیلی کم باشه، مقدار نهایی عدد بزرگی می شه.<br />
به طور خلاصه این روش کمک می کنه کلمات مهم ولی نادر رو در متن پیدا کنید.</p>

<p>مثل روش های قبل دیتا رو می خونیم و یک تابع برای پاکسازی دیتا می نویسیم. این تابع رو مثل روش اول می نویسیم:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def clean_text(text):
  text = "".join([word.lower() for word in text if word not in string.punctuation])
  tokens = re.split('\W+', text)
  text = [ps.stem(word) for word in tokens if word not in stopwords]
  return text
</code></pre></div></div>

<p>و سپس وکتورایز tf-idf رو می سازیم:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from sklearn.feature_extraction.text import TfidfVectorizer

tfidf_vect = TfidfVectorizer(analyzer=clean_text)
X_tfidf = tfidf_vect.fit_transform(dataset['body'])
</code></pre></div></div>

<p>برای اینکه یه دیدی بگیریم بهتره یه بخشش کوچکی از دیتا رو انتخاب کنیم و دیتافریم ماتریس رو بسازیم تا خروجی رو ببینیم. برای ایجاد ماتریس از تابع <code class="language-plaintext highlighter-rouge">toarray()</code> و دیتافریم پانداس استفاده می کنیم:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>X_tfidf_df = pd.DataFrame(X_tfidf_sample.toarray())
</code></pre></div></div>

<p>کد کامل <a href="Uhttps://github.com/spacelover۱/NLP-with-Python/blob/main/۳-VectorizingRawData/TF_IDF.ipynbRL">اینجا</a> قرار داره.</p>

 دقیقه مطالعه -->
    </p>
  </div>

  <p>در این قسمت در مورد برداری کردن دیتا صحبت می کنیم. <br />
تا اینجا دیتا رو خوندیم و تا حدودی نرمالیزه کردیم. الان پایتون دیتا رو فقط یک سری رشته کاراکتر می بینه. حالا برای اینکه مدل ماشین لرنینگ و پایتون این دیتا رو درک کنه باید دیتا برداری بشه. برداری کردن یعنی چی؟ یعنی متن به عددصحیح تبدیل شه و یک بردار ویژگی ساخته شه.<br />
حالا بردار ویژگی در اینجا یعنی متن هر پیام رو بگیریم و به یک بردارعددی تبدیل کنیم که نمایش دهنده متن اون پیام باشه. <br />
چطوری این کار رو انجام می دیم؟ در ادامه درباره این مورد صحبت می کنیم.<br />
چندین روش برای برداری کردن ویژگی ها وجود داره که در ادامه سه روش رایج رو بررسی می کنیم.</p>

<h2 id="روش-اول-بردار-تعداد-count-vectorization"><strong>روش اول: بردار تعداد (Count Vectorization)</strong></h2>

<p>در این روش هر پیام گرفته می شه و هر کلمه به عنوان یک ویژگی در نظر گرفته می شه و بعد تعداد تکرار هر کلمه در اون پیام ثبت می شه. در نهایت یک ماتریسی داریم که هر سطر مربوط به یک پیام و هر ستون نمایش دهنده یک کلمه است. و در نهایت پایتون با بررسی این ماتریس یک ارتباطی بین کلمات موجود در پیام و لیبل اون پیام پیدا می کنه تا در آینده که بهش پیام های بدون لیبل بدیم بتونه به درسی برچسب گذاری کنه.</p>

<p>برای درک بهتر این فرایند به عکس زیر دقت کنید:</p>

<div style="text-align:center"><img src="https://raw.githubusercontent.com/spacelover1/NLP-with-Python/main/3-VectorizingRawData/vectorization_example.PNG" alt="vectorization_example" /></div>

<p>دراین تصویر فقط دو رشته offer و lol از لیست کلمات پیام ها انتخاب شده و تعداد تکرارشون محاسبه شده. همونطور که در جدول سمت چپ و راست می بینید پیام هایی که برچسب غیر اسپم دارند در آن ها رشته lol وجود داشته و تکرار شده ولی شامل رشته offer نیستند و برعکس پیام های اسپم اکثرا رشته offer رو شامل می شن. <br />
این یک مثال بسیار ساده برای درک فرایند و مفهوم بردار تعداد است.</p>

<p>حالا در عمل این روش رو پیاده می کنیم:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import pandas as pd
import re
import string
import nltk

pd.set_option('display.max_colwidth', 100)
dataset = pd.read_csv('SMSSpamCollection.tsv', sep='\t')
dataset.columns = ['label', 'body']

nltk.download('stopwords')
stopwords = nltk.corpus.stopwords.words('english')
ps = nltk.PorterStemmer()


def clean_text(text):
  text = "".join([word.lower() for word in text if word not in string.punctuation])
  tokens = re.split('\W+', text)
  text = [ps.stem(word) for word in tokens if word not in stopwords]
  return text
</code></pre></div></div>

<p>بعد از خوندن و پاکسازی دیتا، سراغ برداری کردن می ریم.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from sklearn.feature_extraction import CountVectorizer

count_vect = CountVectorizer(analyzer=func_name)
X_counts = count_vect.fit_transform(dataset['body'])
</code></pre></div></div>

<p>حالا می تونیم با استفاده از <code class="language-plaintext highlighter-rouge">X_counts.shape</code> تعداد پیام ها و تعداد رشته های منحصر بفرد در این پیام ها رو ببینیم. در این دیتاست 5567 پیام و 8104 رشته منحصر بفرد داریم که همون ویژگی های ما هستند. این اعداد تعداد سطرو و ستون های ماتریس رو نمایش می ده.<br />
و <code class="language-plaintext highlighter-rouge">count_vect.get_feature_names()</code> رشته های منحصربفرد رو نمایش می ده.</p>

<p>تابع هایپرپارامترهای دیگه ای هم داره که <a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html">اینجا</a> می تونید دربارشون بخونید.</p>

<p>حالا در اینجا برای یادگیری 20 پیام اول رو  برداری می کنیم:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sample = dataset[0:20]
count_vect_sample = CountVectorizer(analyzer=clean_text)
X_counts_sample = count_vect_sample.fit_transform(dataset['body'])
</code></pre></div></div>

<p>و الان وقتی سایز دیتای نمونه رو ببینیم 192 رشته منحصربفرد داریم. خروجی و کد کامل این بخش رو <a href="https://github.com/spacelover1/NLP-with-Python/blob/main/3-VectorizingRawData/CountVectorization.ipynb">اینجا</a> می تونید ببینید.</p>

<h2 id="روش-دوم-بردار-n-grams-n-gram-vectorizing"><strong>روش دوم: بردار N-Grams (N-gram vectorizing)</strong></h2>

<p>این روش هم تا حدود زیادی مشابه روش قبلیه و ساختار کدش مشابه اونه. در اینجا هم هر سطر پیام ها هستند ولی هر ستون به جای نمایش یک رشته، ترکیب nتایی از رشته هاست. برای درک بهتر تصویر زیر  رو ببینید:</p>

<div style="text-align:center"><img src="https://raw.githubusercontent.com/spacelover1/NLP-with-Python/main/3-VectorizingRawData/ngrams.png?token=AEGZAVTZYIIT2UNASADUQN3A6KJCE" alt="ngrams" /></div>

<p>مثل مرحله قبل دیتا رو می خونیم و بعد باید تابعی بنویسیم که مراحل پاکسازی رو انجام بده. بخش قبل یک لیستی از توکن رو می دادیم به vectorizer اما الان چون می خواد ترکیبی از کلمه ها رو بسازه باید ورودی بهش یک رشته بدیم. پس در آخر باید توکن ها رو مثل یک جمله کنار هم دیگه قرار بدیم و این کار رو با تابع <code class="language-plaintext highlighter-rouge">join()</code> انجام می دیم:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def clean_text(text):
  text = "".join([word.lower() for word in text if word not in string.punctuation])
  tokens = re.split('\W+', text)
  text = " ".join([ps.stem(word) for word in tokens if word not in stopwords])
  return text
</code></pre></div></div>

<p>اگر یادتون باشه خط اول کاراکترها رو یکی یکی بررسی می کرد برای پیدا کردن علائم نگارشی و در اخر با <code class="language-plaintext highlighter-rouge">join()</code> این کاراکترها رو بهم متصل کردیم. در <code class="language-plaintext highlighter-rouge">join()</code> دومی قرار کلمه ها کنار هم بیان تا جمله بسازن پس باید یک فاصله بین هر کلمه باشه.</p>

<p>در اینجا هم از CountVectorizer استفاده می کنیم:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ngram_vect = CountVectorizer(ngram_range=(2,2))
X_counts = ngram_vect.fit_transform(dataset['cleaned_text'])
</code></pre></div></div>

<p>در <code class="language-plaintext highlighter-rouge">ngram_range</code> مشخص می کنیم که ترکیب چندتایی از کلمه ها بسازه. مثلا (1,3) یعنی همه ترکیبهای یکی، دوتایی و سه تایی. <br />
همونطور که مشاهده می کنید تعداد فیچرها اینجا خیلی زیاد می شه. نکته ای که باید توجه کنیم اینه که چه زمانی از هر کدوم از این روش ها استفاده کنیم. با توجه به مسئله ممکنه یکی از این روش ها نتیجه بهتری بده. لزوما نمی شه گفت یکی از این روش ها بهتر از دیگری است.</p>

<p>کدهای این بخش در <a href="https://github.com/spacelover1/NLP-with-Python/blob/main/3-VectorizingRawData/NGrams.ipynb">اینجا</a> قابل مشاهده است.</p>

<h2 id="روش-سوم-te-idf-term-frequency--inverse-document-frequenct"><strong>روش سوم: TE-IDF (Term Frequency- Inverse Document Frequenct)</strong></h2>

<p>در این روش هم یک ماتریس ایجاد می شه که سطرها پیام ها هستند و هر ستون یک کلمه رو مشخص می کنه. اما سلول های این ماتریس دیگه تعداد تکرار کلمه رو نشون نمی ده بلکه وزن اون کلمه رو نشون می ده، تا اهمیت هر کلمه رو در اون پیام مشخص کنه. <br />
فرمول زیر برای محاسبه این وزنه:</p>

<div style="text-align:center"><img src="https://raw.githubusercontent.com/spacelover1/NLP-with-Python/main/3-VectorizingRawData/tf-idf.PNG?token=AEGZAVSASZLPTSDT75HLYL3A6PNMY" alt="tf-idf_formula" /></div>

<p>بریم ببینیم هر کدوم از این عبارات در فرمول چیو مشخص می کنه و چطوری محاسبه می شه:</p>

<p>عبارت tf تعداد تکرار یک کلمه در یک جمله تقسیم بر تعداد کل کلمات اون جمله. <br />
مثلا در جمله “امروز هوا گرم است” اگر کلمه “گرم” رو در نظر بگیریم، مقدار tf می شه: 1/4 یا 0.25</p>

<p>قسمت دوم این فرمول مشخص می کنه که هر کلمه چند بار تو کل جملات (پیام ها) تکرار شده.
در همین مثال اگر متن ما شامل 20 جمله باشه و کلمه گرم فقط یک بار تکرار شده باشه، نتیجه می شه:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>N = 20, df = 1 &gt;&gt;&gt;&gt; log(N/df) = log(20/1) = 1.301
</code></pre></div></div>

<p>مبنای لگاریتم هم 10 است.</p>

<p>و درنهایت:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0.25 * 1.301 = 0.325
</code></pre></div></div>

<p>هر چقدر مقدار داخل لگاریتم بزرگتر باشه، لگاریتم اون مقدارم بزرگتر می شه. مثلا فرض کنید تعداد کل جملات 40 باشه مقدار لگاریتم می شه 1.6 یعنی بیشتر از مقدار قبل. پس طبق این فرمول هر چقدر یک کلمه در متن کمتر تکرار شده باشه، عددی که تولید می شه بزرگتره. <br />
و اگر یک کلمه در یک جمله خیلی تکرار شده باشه ولی در کل متن خیلی کم باشه، مقدار نهایی عدد بزرگی می شه.<br />
به طور خلاصه این روش کمک می کنه کلمات مهم ولی نادر رو در متن پیدا کنید.</p>

<p>مثل روش های قبل دیتا رو می خونیم و یک تابع برای پاکسازی دیتا می نویسیم. این تابع رو مثل روش اول می نویسیم:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def clean_text(text):
  text = "".join([word.lower() for word in text if word not in string.punctuation])
  tokens = re.split('\W+', text)
  text = [ps.stem(word) for word in tokens if word not in stopwords]
  return text
</code></pre></div></div>

<p>و سپس وکتورایز tf-idf رو می سازیم:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from sklearn.feature_extraction.text import TfidfVectorizer

tfidf_vect = TfidfVectorizer(analyzer=clean_text)
X_tfidf = tfidf_vect.fit_transform(dataset['body'])
</code></pre></div></div>

<p>برای اینکه یه دیدی بگیریم بهتره یه بخشش کوچکی از دیتا رو انتخاب کنیم و دیتافریم ماتریس رو بسازیم تا خروجی رو ببینیم. برای ایجاد ماتریس از تابع <code class="language-plaintext highlighter-rouge">toarray()</code> و دیتافریم پانداس استفاده می کنیم:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>X_tfidf_df = pd.DataFrame(X_tfidf_sample.toarray())
</code></pre></div></div>

<p>کد کامل <a href="Uhttps://github.com/spacelover1/NLP-with-Python/blob/main/3-VectorizingRawData/TF_IDF.ipynbRL">اینجا</a> قرار داره.</p>



  
    <div id="related-posts">
      <h3>مطالب مرتبط</h3>
      <ul>
        
          <li><a href="/the-intimate-universe-one.html">The Intimate Universe(1)</a></li>
        
          <li><a href="/the-weakest-status.html">حالت روحی- گندترین</a></li>
        
          <li><a href="/consistency.html">تولد سه سالگی بلاگ- تداوم</a></li>
        
          <li><a href="/woman.html">زن</a></li>
        
          <li><a href="/for-liberty.html">برای دختری که آرزو داشت پسر بود</a></li>
        
        </ul>
    </div>
  

  <small id="post-tags">
    
      <i class="fas fa-tag"></i>
      <a rel="tag" href="nlp">nlp</a>
    
      <i class="fas fa-tag"></i>
      <a rel="tag" href="اموزش">اموزش</a>
    

    <i class="fas fa-code"></i>
    
    
    <a href="https://raw.githubusercontent.com/spacelover1/personalBlog/master/_posts/2021-07-15-nlp-basics-three.md">سورس</a>
    
  </small>

  <nav class="pagination">
    
      <a href="/nlp-basics-four.html" class="pagination--pager" title="مبانی پردازش زبان طبیعی(NLP)- چهار">قبلی</a>
    

    
      <a href="/nlp-basics-two.html" class="pagination--pager" title="مبانی پردازش زبان طبیعی(NLP)- دو">بعدی </a>
    
  </nav>

  <section>
    
  </section>

</article>
</div>
    </main><footer>

  <div class="wrapper">

    

    <p id="footer-description">هر چیزی به وقتش اتفاق میفته:)
</p>

    <ul class="social-media-list">
      
      <li>
        <i class="fab fa-github"></i>
        <a href="https://github.com/spacelover1">
          <span class="username social-media-text">Github</span>
        </a>
      </li>
      
      
      
      
      <li>
        <i class="fas fa-envelope-open"></i>
        <a href="mailto:spacelover1@gmail.com">
          <span class="username social-media-text">Email</span></a>
      </li>
      
    </ul>

    <div id="footer-extra">
      <small id="license">
  <i class="fab fa-creative-commons"></i>
  مطالب این وبلاگ تحت مجوز
  <a rel="license" href="http://creativecommons.org/licenses/by/4.0/deed.fa">
    کریتیو کامنز اتریبیوشن ۴.۰ اینترنشنال
  </a>قرار دارد.
</small>
      <a id="atom" href="https://validator.w3.org/feed/check.cgi?url=https://spacelover.ir/feed.xml"><img
          src=https://spacelover.ir/assets/img/valid-atom.png alt="[Valid Atom 1.0]"
          title="Validate my Atom 1.0 feed" /></a>
    </div>

  </div>

  <small id="build-time">Site generated on Wed, 30 Nov 2022 21:20:49 +0330</small>

</footer><script src="https://spacelover.ir/assets/js/main.js"></script>
    
  </body>
</html>