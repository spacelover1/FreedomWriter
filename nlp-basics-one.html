<!DOCTYPE html>
<html dir="rtl" lang="fa">
  
  <head><meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta name="author" content="نویسنده" />
<meta name="copyright" content="Commons Attribution 4.0 International" />
<meta name="robot" content="" />
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="description" content="" />

<link rel="canonical" href="https://spacelover.ir/hello-world.html" />
<link rel="icon" href="" />
<link rel="stylesheet" href="https://spacelover.ir/assets/css/main.css" />
<link rel="stylesheet" href="https://spacelover.ir/assets/fonts/fontawesomev5.0.2.css" />
<meta name="keywords" content='nlp,اموزش' /><title>مبانی پردازش زبان طبیعی(NLP)- یک - نویسنده آزاد</title><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>مبانی پردازش زبان طبیعی(NLP)- یک | نویسنده آزاد</title>
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="مبانی پردازش زبان طبیعی(NLP)- یک" />
<meta name="author" content="نویسنده" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="این پست رو با یک نقل قول شروع می کنم: &quot;Motivation often comes after starting, not before. Action produces momentum.&quot;" />
<meta property="og:description" content="این پست رو با یک نقل قول شروع می کنم: &quot;Motivation often comes after starting, not before. Action produces momentum.&quot;" />
<link rel="canonical" href="https://spacelover.ir/nlp-basics-one.html" />
<meta property="og:url" content="https://spacelover.ir/nlp-basics-one.html" />
<meta property="og:site_name" content="نویسنده آزاد" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-07-10T00:00:00+04:30" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="مبانی پردازش زبان طبیعی(NLP)- یک" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"نویسنده"},"dateModified":"2021-07-10T00:00:00+04:30","datePublished":"2021-07-10T00:00:00+04:30","description":"این پست رو با یک نقل قول شروع می کنم: &quot;Motivation often comes after starting, not before. Action produces momentum.&quot;","headline":"مبانی پردازش زبان طبیعی(NLP)- یک","mainEntityOfPage":{"@type":"WebPage","@id":"https://spacelover.ir/nlp-basics-one.html"},"url":"https://spacelover.ir/nlp-basics-one.html"}</script>
<!-- End Jekyll SEO tag -->


  </head>

  <body><header>
  <div class="wrapper">
    <a class="site-title" href="https://spacelover.ir/">نویسنده آزاد</a>
    <small id="motto"> وبلاگ یک دختر ایرانی </small>
  </div>
</header>
<main>
      <div class="wrapper">

<article class="post-content">

  <div class="post-header">
    <h1 class="post-title">مبانی پردازش زبان طبیعی(NLP)- یک</h1>
    <p>
      <i class="fas fa-calendar"></i>
      شنبه ۱۹ تیر ۱۴۰۰<br>
      
<!--       <i class="fas fa-stopwatch"></i>
      <p>این پست رو با یک نقل قول شروع می کنم:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>"Motivation often comes after starting, not before. Action produces momentum."
</code></pre></div></div>

<p>تعاریف متفاوتی برای پردازش زبان طبیعی وجود داره ولی شاید کامل ترینش این باشه:</p>

<blockquote>
  <p>پردازش زبان طبیعی شاخه ای از دانشه که بر روی توانایی کامپیوتر برای درک، تحلیل، تغییر و احتمالا تولید زبان انسان متمرکز است.</p>
</blockquote>

<p>در چند سری پست آینده می خوام یک سری مبانی برنامه نویسی مربوط به پردازش زبان طبیعی رو توضیح بدم، کدها هم در <a href="https://github.com/spacelover۱/NLP-with-Python"><u>این ریپو</u></a> قرار دارند. در این آموزش فرض شده که شما با مبانی پایتون یا حداقل برنامه نویسی آشنا هستید چون اینجا مبانی، مثل استفاده از لیست و دیگر نوع داده ها آموزش داده نمی شه.</p>

<p>زبانی که در این سری استفاده می شه پایتونه، پس نیازه که پایتون رو نصب کنید. برای محیط برنامه نویسی هم می تونید از <a href="https://www.jetbrains.com/pycharm/download/#section=windows"><u>پایچارم</u></a> یا ژوپیتر استفاده کنید. برای استفاده از ژوپیتر باید <a href="https://docs.anaconda.com/anaconda/"><u>آناکوندا</u></a> رو نصب کنید. لینک سایت اصلی این برنامه ها رو هم قرار دادم که برای دانلود می تونید استفاده کنید. حتما دقت کنید که هر برنامه رو برای سیستم عامل خودتون دانلود کنید (خود این سایت ها به صورت خودکار سیستم عامل رو شناسایی می کنند).</p>

<h۲ id="بخش-اول-خواندن-فایل"><strong>بخش اول: خواندن فایل</strong></h۲>

<p>خب شروع کنیم. تو این قسمت قراره یک فایل متنی رو بخونیم و یک سری مرتب سازی ها روش انجام بدیم. اول یک با دیتاست بازی می کنیم تا  یکسری اطلاعات از دیتاست دستمون بیاد و بعد یک روش ساده برای خوندن این دیتاست معرفی می کنیم. <br /> 
فایلی که استفاده می شه در ریپویی که لینکش رو بالا گذاشتم موجوده. برای خوندن فایل باید اول با استفاده از تابع <code class="language-plaintext highlighter-rouge">open()</code> فایل رو باز کنیم و با تابع <code class="language-plaintext highlighter-rouge">read()</code> فایل باز شده رو می خونیم و محتواش رو در یک متغیر به اسم <code class="language-plaintext highlighter-rouge">raw_data</code> می نویسیم.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>raw_data = open('file_name').read()
</code></pre></div></div>

<p>خب حالا باید ببینیم محتوای این <code class="language-plaintext highlighter-rouge">raw_data</code> به چه صورته تا ببینیم نیاز به مرتب سازی داره برای پردازش های بعدی یا نه. فایل رو اگر دیده باشید داده نه ساختاریافته نیست و خیلی هم بدون ساختار نیست، اول هر خط کلمه ham یا spam داره و بعد با یه تب فاصله یک متنی جلوش نوشته شده. بریم چند خط از فایل رو ببینیم:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>raw_data[۰:۵۰۰]
</code></pre></div></div>

<p>بعد از اجرای این خط ۵۰۰ کاراکتر اول فایل نمایش داده می شه. همون طور که می بینید محتوای فایل یک سری رشته است که کاراکترهایی مثل <code class="language-plaintext highlighter-rouge">\t</code> و <code class="language-plaintext highlighter-rouge">\n</code> داره. اولی فاصلی ای مه باتب ایجاد شده رو نشون می ده و دومی باعث می شه بره خط بعدی.
خب ما الان می خوایم یکم داده رو مرتب کنیم، برای پردازش های بعدی. برای اینکار کاراکتر <code class="language-plaintext highlighter-rouge">\t</code> رو با <code class="language-plaintext highlighter-rouge">\n</code> جایگزین می کنیم و بعد با تابع <code class="language-plaintext highlighter-rouge">split</code> این رشته رو به لیست تبدیل می کنیم.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>parsed_data = raw_data.replace('\t','\n').split('\n')
</code></pre></div></div>

<p>بریم ببینیم لیست مون چه شکلیه:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>parsed_data[:۵]
</code></pre></div></div>

<p>الان برچسب یا لیبل در خونه های با ایندکس زوج قرار داره و متن پیام در خونه های ایندکس فرد. حالا که یه نظمی تو ساختار ایجاد شده بریم هر کدوم رو تو لیست جدا بنویسیم:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>label_list  = parsed_data[۰::۲]
text_list = parsed_data[۱::۲]
</code></pre></div></div>

<p>اولی لیست رو از خونه صفر می خونه و یکی درمیون مقادیر رو می ریزه تو لیست لیبلا، یعنی همون خونه های زوج. خط دوم از خونه یک میاد یکی در میون می خونه و باعث می شه خونه های فرد رو بخونه.<br />
می تونیم این دو لیست رو ببینیم و مطمئن شیم همونطور که می خواستیم شده. اینجا از تابع <code class="language-plaintext highlighter-rouge">print()</code> استفاده می کنیم چون در غیر اینصورت ژوپیتر فقط خروجی اخرین خط رو می ده.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>print(label_list[:۵])
print(text_list[:۵])
</code></pre></div></div>

<p>حالا باید این دو لیست رو یه جوری با هم ترکیب کنیم تا برای تحلیل های بعدی بتونیم ازشون استفاده کنیم. برای این کار از تابع <code class="language-plaintext highlighter-rouge">DataFrame()</code> پانداس استفاده می کنیم و در این دیتافریم یک دیکشنری می سازیم. بعد از ایمپورت کتابخونه می تونیم از توابعش استفاده کنیم:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import pandas as pd
full_corpus = DataFrame({'label': label_list, 'body': text_list})
</code></pre></div></div>

<p>احتمالا بعد از اجرای این خط کد اروری مشاهده می کنید که می گه ارایه ها باید طولشون یکسان باشه. بریم طول این دو لیست رو چک کنیم:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>print(len(label_list))
print(len(text_list))
</code></pre></div></div>

<p>همون طور که می بینید طول لیست لیبل یه دونه بیشتر از متنه. بریم چند تا خونه اخر هر دو لیست رو ببینیم چه خبره:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>label_list [-۵:]
text_list[-۵:]
</code></pre></div></div>

<p>خونه اخر لیست لیبلا یه خونه خالیه. پس وقتی این لیست رو می خونیم، خونه اخر رو باید نادیده بگیریم:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>full_corpus = DataFrame({'label': label_list[:-۱], 'body': text_list})
</code></pre></div></div>

<p>ببینیم دیتافریم مون چه شکلیه:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>full_corpus.head()
</code></pre></div></div>

<p>خب الان دیتامون یکم ساختار پیدا کرده و نسبت به اول خوانایی بهتری داره. <br /></p>

<p>و اما روش ساده خوندن این دیتاست. همون اول که نگاهی به داده انداختیم و <code class="language-plaintext highlighter-rouge">\t</code> رو دیدیم باید متوجه شیم که این یک فایلیه که عناصرش با تب از هم جدا شدن. برای خوندن این فایل ها می تونیم از تابع <code class="language-plaintext highlighter-rouge">read_csv</code> استفاده کنیم، مقدار هدر رو هم در اینجا <code class="language-plaintext highlighter-rouge">None</code> قرار می دیم چون در فایل اصلی ردیفی برای عنوان ستون وجود نداره.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>dataset = read_csv('file_name', sep="\t", header=None)
</code></pre></div></div>

<p>برای مشاهده هم می تونیم از تابع <code class="language-plaintext highlighter-rouge">head()</code> استفاده کنیم:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>dataset.head()
</code></pre></div></div>

<p>همونطور که می بینید دیگه ردیف اولی برای عنوان ستون ها وجود نداره.</p>

<p>در این بخش یاد گرفتیم که یک دیتاست متنی رو چطوری بخونیم و برای تحلیل های بعدی مرتبش کنیم.
کدهای بخش اول رو از <a href="https://github.com/spacelover۱/NLP-with-Python/blob/main/۱-Basics/۰۱-read_file.ipynb"><u>اینجا</u></a> می تونید ببینید.</p>

<h۲ id="بخش-دوم-بررسی-دیتاست"><strong>بخش دوم: بررسی دیتاست</strong></h۲>

<p>بخش قبل یاد گرفتیم که چطور دیتا رو به روش پیچیده بخونیم تا با یک سری از ابزارهای تغییر در متن آشنا شیم و در آخر روش ساده رو یاد گرفتیم:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>dataset = read_csv('file_name', sep="\t", header=None)
</code></pre></div></div>

<p>حالا برای ستون ها اسم برچسب می ذاریم تا راحت بتونیم دیتا رو بررسی کنیم:
    dataset.columns[‘label’, ‘body’]</p>

<p>اول از همه بریم سایز دیتاست رو دربیاریم، ببینیم چند تا سطر و ستون داره:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>print("There are {} rows and {} columns.".format(len(dataset), len(dataset.columns)))
</code></pre></div></div>

<p>حالا باید تعداد سطرهای اسپم و غیر اسپم رو دربیاریم:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>print("Out of {} rows, {} rows are spam and {} rows are ham.".format(
                                                                    len(dataset),
                                                                    len(dataset[dataset['label'] == 'spam']),
                                                                    len(dataset[dataset['label'] == 'hame'])
                                                                    ))
</code></pre></div></div>

<p>ممکنه بعضی ردیفا هیچ لیبلی نداشته باشه، باید تعداد اون ها رو هم پیدا کنیم:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>print("Number of null in label: {}".format(dataset['label'].isnull().sum()))
print("Number of null in body: {}".format(dataset['body'].isnull().sum()))
</code></pre></div></div>

<p>خروجی تابع <code class="language-plaintext highlighter-rouge">isnull()</code> بولینه یعنی True یا False برمی گردونه. هر سطر رو بررسی می کنه اگر اون ستونی که داریم بررسی می کنیم تو اون سطر دیتا نداشته باشه True میشه و در غیراینصورت False. 
وقتی بعد از <code class="language-plaintext highlighter-rouge">isnull()</code> تابع <code class="language-plaintext highlighter-rouge">sum()</code> رو میاریم یعنی تعداد کل ریف هایی که لیبل ندارن رو بده.</p>

<p>کدهای بخش دوم رو از <a href="https://github.com/spacelover۱/NLP-with-Python/blob/main/۱-Basics/۰۲-Exploring%۲۰Dataset.ipynb"><u>اینجا</u></a> می تونید ببینید.</p>

<h۲ id="بخش-سوم-عبارات-منظم"><strong>بخش سوم: عبارات منظم</strong></h۲>

<p>دلیل اصلی یاد گرفتن عبارات با قاعده اینه که بتونیم جمله رو tokenize کنیم یا به عبارتی کلمه های جمله رو از هم جدا کنیم که پایتون بدونه که دنبال چی بگرده. گاهی اوقات لازمه که یک الگوی خاصی از کاراکترها رو در یک رشته متن پیدا کنیم، این کار به راحتی با عبارات با قاعده قابل انجامه. البته خیلی راحتم نه :) چون روش هایی که برای تشخیص این الگوها وجود داره خیلی گسترده است ولی یک سری موارد کلی رو اگر یاد بگیرید و تمرین کنید براتون راحت می شه. از <a href="https://docs.python.org/۳/library/re.html">داکیومنت عبارات با قاعده در پایتون</a> هم می تونید استفاده کنید.</p>

<p>در اینجا دو روش رو یاد می گیریم. برای تست این دو روش یک متن رو به سه حالت نوشتم و قراره که کلمه ها رو از هم جدا کنیم.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import re
text = "This is a made up string to test ۲ different regex methods"
messy_text = "This            is a made up string to              test ۲   different regex methods"
messy_text۱ = "This-is-a''''made-up/string-+to*****test-۲ &gt;&gt;&gt;&gt;&gt;&gt;-different.regex-methods"
</code></pre></div></div>

<p>برای اینکه بتونیم از این عبارات با قاعده در پایتون استفاده کنیم باید کتابخونه ش رو ایمپورت کنیم. <br />
کلمات جمله اول با فاصله از هم جدا شدند پس یه الگو به دست اومد، می تونیم هر جا کاراکتر فاصله بود تشخیص بدیم و کلمات رو از هم جدا کنیم. با استفاده از تابع <code class="language-plaintext highlighter-rouge">split()</code> و ‘\s’ به کوچک و بزرگ بودن حروف دقت کنید، چون هر کدوم معنی خاصی دارند. در اینجا حرف کوچک s یعنی از نقاطی که کاراکتر فاصله وجود داره کلمات رو جدا کن.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>re.split('\s', 'text')
</code></pre></div></div>

<p>اگر همین کد رو روی متن دوم اجرا کنیم جوابی که می خوایم رو نمی گیریم، چون بیشتر از یک کاراکتر فاصله بین بعضی کلمات وجود داره. پس از ‘\s+’ استفاده می کنیم. این یعنی یک فاصله یا بیشتر اگر بین کلمات بود حذف کن.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>re.split('\s+', 'messy_text')
</code></pre></div></div>

<p>برای متن سوم هیچ کدوم از راه حلای بالا جواب نمی ده. و باید از راه دیگه ای بریم. اینجا به غیر از کاراکتر فاصله، کاراکترهای دیگه ای هم وجود داره. پس می تونیم بیاییم بگیم هر کاراکتر غیر کلمه ای رو حذف کن. و علامت + رو هم می ذاریم چون کاراکترهای غیرکلمه بیشتر از یکبار تکرار شدن بین هر کلمه.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>re.split('\W+', 'messy_text۱')
</code></pre></div></div>

<p>کاری که تا الان انجام می دادیم این بود که بیاییم هر چی غیر از کلمه است رو جدا می کردیم تا کلمه ها جدا شن. یه راه دیگه برای جداسازی کلمات اینه که بیاییم مستقیم کلمه ها رو پیدا کنیم. برای این کار از تابع <code class="language-plaintext highlighter-rouge">findall()</code> استفاده می کنیم.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>re.findall('\S+', text)
</code></pre></div></div>

<p>در کد بالا از حرف بزرگ S استفاده کردیم. گفتیم هر چی غیر کاراکتر فاصله. که این روش دوباره روی متن سوم جواب نمیده. که می تونیم از روش زیر به جای این استفاده کنیم.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>re.findall('\w+', 'messy_text۱')
</code></pre></div></div>

<p>حرف کوچک w رو اینجا استفاده کردیم. یعنی کاراکترهای کلمه رو جدا کن.</p>

<p>کدهای بخش سوم <a href="https://github.com/spacelover۱/NLP-with-Python/blob/main/۱-Basics/۰۳-RegEx.ipynb"><u>اینجا</u></a> قرار دارند.<br />
یک کار دیگه ای که می شه با این عبارات با قاعده انجام داد اینه که کلماتی که املاشون اشتباه نوشته شده در یک متن رو پیدا کرد و با مقدار درستش جایگزین کرد. مثال این مورد رو می تونید در <a href="https://github.com/spacelover۱/NLP-with-Python/blob/main/۱-Basics/۰۴-RegEx۱.ipynb">اینجا</a> مشاهده کنید.</p>

<h۲ id="بخش-چهارم-پیش-پردازش-پاکسازی-داده"><strong>بخش چهارم: پیش پردازش (پاکسازی) داده</strong></h۲>

<p>برای اینکه داده آماده بررسی و تحلیل بشه باید یکسری مراحل به عنوان پیش پردازش روش انجام بشه تا مواردی که اضافه است حذف بشه. این مراحل شامل حذف علائم نگارشی، تقسیم جمله به کلمه ها متشکل، حذف کلماتی که معنی خاصی ندارندو حذف مشتقات کلمات می شود.</p>

<p>بریم پنج سطر اول دیتا رو دوباره ببینیم:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import pandas as pd
pd.set_option('display.max_colwidth', ۱۰۰)
dataset = pd.read_csv('file_name', sep='\t', header=None)
dataset.columns = ['label', 'body']
dataset.head()
</code></pre></div></div>

<p>خط دوم <code class="language-plaintext highlighter-rouge">set_option('display.max_colwidth', ۱۰۰)</code> تعداد کاراکترهایی که نمایش داده می شه رو مشخص می کنه. <br />
الان دیتا به این صورته:</p>

<p><img src="https://raw.githubusercontent.com/spacelover۱/personalBlog/master/image/dataset.PNG" alt="dataset_before_cleaning" /></p>

<p>فایل دیتاست بعد از پاکسازی هم در این فولدر وجود داره و قراره دیتا به این صورت دربیاد:</p>

<p><img src="https://raw.githubusercontent.com/spacelover۱/personalBlog/master/image/cleaned_dataset.PNG" alt="cleaned_dataset" /></p>

<h۳ id="حذف-علائم-نگارشی"><strong>حذف علائم نگارشی</strong></h۳>

<p>علائم نگارشی در کتابخونه <code class="language-plaintext highlighter-rouge">string</code> قرار دارند. باید یک تابع بنویسیم که این علائم نگارشی رو از متن پیام در دیتا حذف کنه. و متن بدون علائم نگارشی بده.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def remove_punct(text):
    text_nopunct = [char for char in text if char not in string.punctuation]
    return text_nopunct

dataset['body_nopunct'] = dataset['body'].apply(lambda x: remove_punct(x))
</code></pre></div></div>

<p>در اینجا چون متن پیام رو داره کاراکتر  به کاراکتر بررسی می کنه در نهایت هم کاراکتر ها رو از هم جدا می کنه و به عنوان خروجی می ده، برای اینکه به صورت کلمه خروجی بگیریم از تابع <code class="language-plaintext highlighter-rouge">join()</code> استفاده می کنیم.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>text_nopunct = "".join([char for char in text if char not in string.punctuation])
</code></pre></div></div>

<h۳ id="جداسازی-کلمات"><strong>جداسازی کلمات</strong></h۳>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import re
def tokenize(text):
    tokens = re.split('\W+', text)
    return tokens
    
dataset['body_tokenized'] = dataset['body_nopunct'].apply(lambda x: tokenize(x.lower()))
</code></pre></div></div>

<p>تابع <code class="language-plaintext highlighter-rouge">lower()</code> شاید اینجا زیاد استفاده نشه و اهمیتش مشخص نباشه ولی چون در پایتون حروف کوچک و بزرگ یکسان نیستند باید همه حروف در کلمات یک جور باشند.</p>

<h۳ id="حذف-کلمات-بدون-معنی"><strong>حذف کلمات بدون معنی</strong></h۳>

<p>هر زبانی یک سری کلمات داره که معنی خاصی در جمله ندارند و اگر حذف شوند جمله معنی خودش رو حفظ می کنه. مثل حروف ربط. با استفاده از کتابخانه <code class="language-plaintext highlighter-rouge">nltk</code> این کلمات رو از جمله حذف می کنیم:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import nltk
stopwords = nltk.corpus.stopwords.words('english')
</code></pre></div></div>

<p>حالا یک تابع می نویسیم که این کلمات رو حذف کنه از جملات:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def remove_stopwords(tokenized_list):
    text_nostop = [word for word in tokenized_list if word not in stopwords]
    return text_nostop

dataset['body_nostop'] = dataset['body_tokenized'].apply(lambda x: remove_stopwords(x))
</code></pre></div></div>

<p>کدهای بخش چهارم <a href="https://github.com/spacelover۱/NLP-with-Python/blob/main/۱-Basics/۰۵-cleaning%۲۰text_۱.ipynb"><u>اینجا</u></a> قرار دارند.</p>

 دقیقه مطالعه -->
    </p>
  </div>

  <p>این پست رو با یک نقل قول شروع می کنم:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>"Motivation often comes after starting, not before. Action produces momentum."
</code></pre></div></div>

<p>تعاریف متفاوتی برای پردازش زبان طبیعی وجود داره ولی شاید کامل ترینش این باشه:</p>

<blockquote>
  <p>پردازش زبان طبیعی شاخه ای از دانشه که بر روی توانایی کامپیوتر برای درک، تحلیل، تغییر و احتمالا تولید زبان انسان متمرکز است.</p>
</blockquote>

<p>در چند سری پست آینده می خوام یک سری مبانی برنامه نویسی مربوط به پردازش زبان طبیعی رو توضیح بدم، کدها هم در <a href="https://github.com/spacelover1/NLP-with-Python"><u>این ریپو</u></a> قرار دارند. در این آموزش فرض شده که شما با مبانی پایتون یا حداقل برنامه نویسی آشنا هستید چون اینجا مبانی، مثل استفاده از لیست و دیگر نوع داده ها آموزش داده نمی شه.</p>

<p>زبانی که در این سری استفاده می شه پایتونه، پس نیازه که پایتون رو نصب کنید. برای محیط برنامه نویسی هم می تونید از <a href="https://www.jetbrains.com/pycharm/download/#section=windows"><u>پایچارم</u></a> یا ژوپیتر استفاده کنید. برای استفاده از ژوپیتر باید <a href="https://docs.anaconda.com/anaconda/"><u>آناکوندا</u></a> رو نصب کنید. لینک سایت اصلی این برنامه ها رو هم قرار دادم که برای دانلود می تونید استفاده کنید. حتما دقت کنید که هر برنامه رو برای سیستم عامل خودتون دانلود کنید (خود این سایت ها به صورت خودکار سیستم عامل رو شناسایی می کنند).</p>

<h2 id="بخش-اول-خواندن-فایل"><strong>بخش اول: خواندن فایل</strong></h2>

<p>خب شروع کنیم. تو این قسمت قراره یک فایل متنی رو بخونیم و یک سری مرتب سازی ها روش انجام بدیم. اول یک با دیتاست بازی می کنیم تا  یکسری اطلاعات از دیتاست دستمون بیاد و بعد یک روش ساده برای خوندن این دیتاست معرفی می کنیم. <br /> 
فایلی که استفاده می شه در ریپویی که لینکش رو بالا گذاشتم موجوده. برای خوندن فایل باید اول با استفاده از تابع <code class="language-plaintext highlighter-rouge">open()</code> فایل رو باز کنیم و با تابع <code class="language-plaintext highlighter-rouge">read()</code> فایل باز شده رو می خونیم و محتواش رو در یک متغیر به اسم <code class="language-plaintext highlighter-rouge">raw_data</code> می نویسیم.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>raw_data = open('file_name').read()
</code></pre></div></div>

<p>خب حالا باید ببینیم محتوای این <code class="language-plaintext highlighter-rouge">raw_data</code> به چه صورته تا ببینیم نیاز به مرتب سازی داره برای پردازش های بعدی یا نه. فایل رو اگر دیده باشید داده نه ساختاریافته نیست و خیلی هم بدون ساختار نیست، اول هر خط کلمه ham یا spam داره و بعد با یه تب فاصله یک متنی جلوش نوشته شده. بریم چند خط از فایل رو ببینیم:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>raw_data[0:500]
</code></pre></div></div>

<p>بعد از اجرای این خط 500 کاراکتر اول فایل نمایش داده می شه. همون طور که می بینید محتوای فایل یک سری رشته است که کاراکترهایی مثل <code class="language-plaintext highlighter-rouge">\t</code> و <code class="language-plaintext highlighter-rouge">\n</code> داره. اولی فاصلی ای مه باتب ایجاد شده رو نشون می ده و دومی باعث می شه بره خط بعدی.
خب ما الان می خوایم یکم داده رو مرتب کنیم، برای پردازش های بعدی. برای اینکار کاراکتر <code class="language-plaintext highlighter-rouge">\t</code> رو با <code class="language-plaintext highlighter-rouge">\n</code> جایگزین می کنیم و بعد با تابع <code class="language-plaintext highlighter-rouge">split</code> این رشته رو به لیست تبدیل می کنیم.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>parsed_data = raw_data.replace('\t','\n').split('\n')
</code></pre></div></div>

<p>بریم ببینیم لیست مون چه شکلیه:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>parsed_data[:5]
</code></pre></div></div>

<p>الان برچسب یا لیبل در خونه های با ایندکس زوج قرار داره و متن پیام در خونه های ایندکس فرد. حالا که یه نظمی تو ساختار ایجاد شده بریم هر کدوم رو تو لیست جدا بنویسیم:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>label_list  = parsed_data[0::2]
text_list = parsed_data[1::2]
</code></pre></div></div>

<p>اولی لیست رو از خونه صفر می خونه و یکی درمیون مقادیر رو می ریزه تو لیست لیبلا، یعنی همون خونه های زوج. خط دوم از خونه یک میاد یکی در میون می خونه و باعث می شه خونه های فرد رو بخونه.<br />
می تونیم این دو لیست رو ببینیم و مطمئن شیم همونطور که می خواستیم شده. اینجا از تابع <code class="language-plaintext highlighter-rouge">print()</code> استفاده می کنیم چون در غیر اینصورت ژوپیتر فقط خروجی اخرین خط رو می ده.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>print(label_list[:5])
print(text_list[:5])
</code></pre></div></div>

<p>حالا باید این دو لیست رو یه جوری با هم ترکیب کنیم تا برای تحلیل های بعدی بتونیم ازشون استفاده کنیم. برای این کار از تابع <code class="language-plaintext highlighter-rouge">DataFrame()</code> پانداس استفاده می کنیم و در این دیتافریم یک دیکشنری می سازیم. بعد از ایمپورت کتابخونه می تونیم از توابعش استفاده کنیم:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import pandas as pd
full_corpus = DataFrame({'label': label_list, 'body': text_list})
</code></pre></div></div>

<p>احتمالا بعد از اجرای این خط کد اروری مشاهده می کنید که می گه ارایه ها باید طولشون یکسان باشه. بریم طول این دو لیست رو چک کنیم:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>print(len(label_list))
print(len(text_list))
</code></pre></div></div>

<p>همون طور که می بینید طول لیست لیبل یه دونه بیشتر از متنه. بریم چند تا خونه اخر هر دو لیست رو ببینیم چه خبره:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>label_list [-5:]
text_list[-5:]
</code></pre></div></div>

<p>خونه اخر لیست لیبلا یه خونه خالیه. پس وقتی این لیست رو می خونیم، خونه اخر رو باید نادیده بگیریم:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>full_corpus = DataFrame({'label': label_list[:-1], 'body': text_list})
</code></pre></div></div>

<p>ببینیم دیتافریم مون چه شکلیه:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>full_corpus.head()
</code></pre></div></div>

<p>خب الان دیتامون یکم ساختار پیدا کرده و نسبت به اول خوانایی بهتری داره. <br /></p>

<p>و اما روش ساده خوندن این دیتاست. همون اول که نگاهی به داده انداختیم و <code class="language-plaintext highlighter-rouge">\t</code> رو دیدیم باید متوجه شیم که این یک فایلیه که عناصرش با تب از هم جدا شدن. برای خوندن این فایل ها می تونیم از تابع <code class="language-plaintext highlighter-rouge">read_csv</code> استفاده کنیم، مقدار هدر رو هم در اینجا <code class="language-plaintext highlighter-rouge">None</code> قرار می دیم چون در فایل اصلی ردیفی برای عنوان ستون وجود نداره.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>dataset = read_csv('file_name', sep="\t", header=None)
</code></pre></div></div>

<p>برای مشاهده هم می تونیم از تابع <code class="language-plaintext highlighter-rouge">head()</code> استفاده کنیم:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>dataset.head()
</code></pre></div></div>

<p>همونطور که می بینید دیگه ردیف اولی برای عنوان ستون ها وجود نداره.</p>

<p>در این بخش یاد گرفتیم که یک دیتاست متنی رو چطوری بخونیم و برای تحلیل های بعدی مرتبش کنیم.
کدهای بخش اول رو از <a href="https://github.com/spacelover1/NLP-with-Python/blob/main/1-Basics/01-read_file.ipynb"><u>اینجا</u></a> می تونید ببینید.</p>

<h2 id="بخش-دوم-بررسی-دیتاست"><strong>بخش دوم: بررسی دیتاست</strong></h2>

<p>بخش قبل یاد گرفتیم که چطور دیتا رو به روش پیچیده بخونیم تا با یک سری از ابزارهای تغییر در متن آشنا شیم و در آخر روش ساده رو یاد گرفتیم:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>dataset = read_csv('file_name', sep="\t", header=None)
</code></pre></div></div>

<p>حالا برای ستون ها اسم برچسب می ذاریم تا راحت بتونیم دیتا رو بررسی کنیم:
    dataset.columns[‘label’, ‘body’]</p>

<p>اول از همه بریم سایز دیتاست رو دربیاریم، ببینیم چند تا سطر و ستون داره:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>print("There are {} rows and {} columns.".format(len(dataset), len(dataset.columns)))
</code></pre></div></div>

<p>حالا باید تعداد سطرهای اسپم و غیر اسپم رو دربیاریم:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>print("Out of {} rows, {} rows are spam and {} rows are ham.".format(
                                                                    len(dataset),
                                                                    len(dataset[dataset['label'] == 'spam']),
                                                                    len(dataset[dataset['label'] == 'hame'])
                                                                    ))
</code></pre></div></div>

<p>ممکنه بعضی ردیفا هیچ لیبلی نداشته باشه، باید تعداد اون ها رو هم پیدا کنیم:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>print("Number of null in label: {}".format(dataset['label'].isnull().sum()))
print("Number of null in body: {}".format(dataset['body'].isnull().sum()))
</code></pre></div></div>

<p>خروجی تابع <code class="language-plaintext highlighter-rouge">isnull()</code> بولینه یعنی True یا False برمی گردونه. هر سطر رو بررسی می کنه اگر اون ستونی که داریم بررسی می کنیم تو اون سطر دیتا نداشته باشه True میشه و در غیراینصورت False. 
وقتی بعد از <code class="language-plaintext highlighter-rouge">isnull()</code> تابع <code class="language-plaintext highlighter-rouge">sum()</code> رو میاریم یعنی تعداد کل ریف هایی که لیبل ندارن رو بده.</p>

<p>کدهای بخش دوم رو از <a href="https://github.com/spacelover1/NLP-with-Python/blob/main/1-Basics/02-Exploring%20Dataset.ipynb"><u>اینجا</u></a> می تونید ببینید.</p>

<h2 id="بخش-سوم-عبارات-منظم"><strong>بخش سوم: عبارات منظم</strong></h2>

<p>دلیل اصلی یاد گرفتن عبارات با قاعده اینه که بتونیم جمله رو tokenize کنیم یا به عبارتی کلمه های جمله رو از هم جدا کنیم که پایتون بدونه که دنبال چی بگرده. گاهی اوقات لازمه که یک الگوی خاصی از کاراکترها رو در یک رشته متن پیدا کنیم، این کار به راحتی با عبارات با قاعده قابل انجامه. البته خیلی راحتم نه :) چون روش هایی که برای تشخیص این الگوها وجود داره خیلی گسترده است ولی یک سری موارد کلی رو اگر یاد بگیرید و تمرین کنید براتون راحت می شه. از <a href="https://docs.python.org/3/library/re.html">داکیومنت عبارات با قاعده در پایتون</a> هم می تونید استفاده کنید.</p>

<p>در اینجا دو روش رو یاد می گیریم. برای تست این دو روش یک متن رو به سه حالت نوشتم و قراره که کلمه ها رو از هم جدا کنیم.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import re
text = "This is a made up string to test 2 different regex methods"
messy_text = "This            is a made up string to              test 2   different regex methods"
messy_text1 = "This-is-a''''made-up/string-+to*****test-2 &gt;&gt;&gt;&gt;&gt;&gt;-different.regex-methods"
</code></pre></div></div>

<p>برای اینکه بتونیم از این عبارات با قاعده در پایتون استفاده کنیم باید کتابخونه ش رو ایمپورت کنیم. <br />
کلمات جمله اول با فاصله از هم جدا شدند پس یه الگو به دست اومد، می تونیم هر جا کاراکتر فاصله بود تشخیص بدیم و کلمات رو از هم جدا کنیم. با استفاده از تابع <code class="language-plaintext highlighter-rouge">split()</code> و ‘\s’ به کوچک و بزرگ بودن حروف دقت کنید، چون هر کدوم معنی خاصی دارند. در اینجا حرف کوچک s یعنی از نقاطی که کاراکتر فاصله وجود داره کلمات رو جدا کن.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>re.split('\s', 'text')
</code></pre></div></div>

<p>اگر همین کد رو روی متن دوم اجرا کنیم جوابی که می خوایم رو نمی گیریم، چون بیشتر از یک کاراکتر فاصله بین بعضی کلمات وجود داره. پس از ‘\s+’ استفاده می کنیم. این یعنی یک فاصله یا بیشتر اگر بین کلمات بود حذف کن.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>re.split('\s+', 'messy_text')
</code></pre></div></div>

<p>برای متن سوم هیچ کدوم از راه حلای بالا جواب نمی ده. و باید از راه دیگه ای بریم. اینجا به غیر از کاراکتر فاصله، کاراکترهای دیگه ای هم وجود داره. پس می تونیم بیاییم بگیم هر کاراکتر غیر کلمه ای رو حذف کن. و علامت + رو هم می ذاریم چون کاراکترهای غیرکلمه بیشتر از یکبار تکرار شدن بین هر کلمه.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>re.split('\W+', 'messy_text1')
</code></pre></div></div>

<p>کاری که تا الان انجام می دادیم این بود که بیاییم هر چی غیر از کلمه است رو جدا می کردیم تا کلمه ها جدا شن. یه راه دیگه برای جداسازی کلمات اینه که بیاییم مستقیم کلمه ها رو پیدا کنیم. برای این کار از تابع <code class="language-plaintext highlighter-rouge">findall()</code> استفاده می کنیم.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>re.findall('\S+', text)
</code></pre></div></div>

<p>در کد بالا از حرف بزرگ S استفاده کردیم. گفتیم هر چی غیر کاراکتر فاصله. که این روش دوباره روی متن سوم جواب نمیده. که می تونیم از روش زیر به جای این استفاده کنیم.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>re.findall('\w+', 'messy_text1')
</code></pre></div></div>

<p>حرف کوچک w رو اینجا استفاده کردیم. یعنی کاراکترهای کلمه رو جدا کن.</p>

<p>کدهای بخش سوم <a href="https://github.com/spacelover1/NLP-with-Python/blob/main/1-Basics/03-RegEx.ipynb"><u>اینجا</u></a> قرار دارند.<br />
یک کار دیگه ای که می شه با این عبارات با قاعده انجام داد اینه که کلماتی که املاشون اشتباه نوشته شده در یک متن رو پیدا کرد و با مقدار درستش جایگزین کرد. مثال این مورد رو می تونید در <a href="https://github.com/spacelover1/NLP-with-Python/blob/main/1-Basics/04-RegEx1.ipynb">اینجا</a> مشاهده کنید.</p>

<h2 id="بخش-چهارم-پیش-پردازش-پاکسازی-داده"><strong>بخش چهارم: پیش پردازش (پاکسازی) داده</strong></h2>

<p>برای اینکه داده آماده بررسی و تحلیل بشه باید یکسری مراحل به عنوان پیش پردازش روش انجام بشه تا مواردی که اضافه است حذف بشه. این مراحل شامل حذف علائم نگارشی، تقسیم جمله به کلمه ها متشکل، حذف کلماتی که معنی خاصی ندارندو حذف مشتقات کلمات می شود.</p>

<p>بریم پنج سطر اول دیتا رو دوباره ببینیم:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import pandas as pd
pd.set_option('display.max_colwidth', 100)
dataset = pd.read_csv('file_name', sep='\t', header=None)
dataset.columns = ['label', 'body']
dataset.head()
</code></pre></div></div>

<p>خط دوم <code class="language-plaintext highlighter-rouge">set_option('display.max_colwidth', 100)</code> تعداد کاراکترهایی که نمایش داده می شه رو مشخص می کنه. <br />
الان دیتا به این صورته:</p>

<p><img src="https://raw.githubusercontent.com/spacelover1/personalBlog/master/image/dataset.PNG" alt="dataset_before_cleaning" /></p>

<p>فایل دیتاست بعد از پاکسازی هم در این فولدر وجود داره و قراره دیتا به این صورت دربیاد:</p>

<p><img src="https://raw.githubusercontent.com/spacelover1/personalBlog/master/image/cleaned_dataset.PNG" alt="cleaned_dataset" /></p>

<h3 id="حذف-علائم-نگارشی"><strong>حذف علائم نگارشی</strong></h3>

<p>علائم نگارشی در کتابخونه <code class="language-plaintext highlighter-rouge">string</code> قرار دارند. باید یک تابع بنویسیم که این علائم نگارشی رو از متن پیام در دیتا حذف کنه. و متن بدون علائم نگارشی بده.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def remove_punct(text):
    text_nopunct = [char for char in text if char not in string.punctuation]
    return text_nopunct

dataset['body_nopunct'] = dataset['body'].apply(lambda x: remove_punct(x))
</code></pre></div></div>

<p>در اینجا چون متن پیام رو داره کاراکتر  به کاراکتر بررسی می کنه در نهایت هم کاراکتر ها رو از هم جدا می کنه و به عنوان خروجی می ده، برای اینکه به صورت کلمه خروجی بگیریم از تابع <code class="language-plaintext highlighter-rouge">join()</code> استفاده می کنیم.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>text_nopunct = "".join([char for char in text if char not in string.punctuation])
</code></pre></div></div>

<h3 id="جداسازی-کلمات"><strong>جداسازی کلمات</strong></h3>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import re
def tokenize(text):
    tokens = re.split('\W+', text)
    return tokens
    
dataset['body_tokenized'] = dataset['body_nopunct'].apply(lambda x: tokenize(x.lower()))
</code></pre></div></div>

<p>تابع <code class="language-plaintext highlighter-rouge">lower()</code> شاید اینجا زیاد استفاده نشه و اهمیتش مشخص نباشه ولی چون در پایتون حروف کوچک و بزرگ یکسان نیستند باید همه حروف در کلمات یک جور باشند.</p>

<h3 id="حذف-کلمات-بدون-معنی"><strong>حذف کلمات بدون معنی</strong></h3>

<p>هر زبانی یک سری کلمات داره که معنی خاصی در جمله ندارند و اگر حذف شوند جمله معنی خودش رو حفظ می کنه. مثل حروف ربط. با استفاده از کتابخانه <code class="language-plaintext highlighter-rouge">nltk</code> این کلمات رو از جمله حذف می کنیم:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import nltk
stopwords = nltk.corpus.stopwords.words('english')
</code></pre></div></div>

<p>حالا یک تابع می نویسیم که این کلمات رو حذف کنه از جملات:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def remove_stopwords(tokenized_list):
    text_nostop = [word for word in tokenized_list if word not in stopwords]
    return text_nostop

dataset['body_nostop'] = dataset['body_tokenized'].apply(lambda x: remove_stopwords(x))
</code></pre></div></div>

<p>کدهای بخش چهارم <a href="https://github.com/spacelover1/NLP-with-Python/blob/main/1-Basics/05-cleaning%20text_1.ipynb"><u>اینجا</u></a> قرار دارند.</p>



  
    <div id="related-posts">
      <h3>مطالب مرتبط</h3>
      <ul>
        
          <li><a href="/cmon-cmon.html">فیلم c'mon, c'mon</a></li>
        
          <li><a href="/big-picture.html">Big Picture, Please!</a></li>
        
          <li><a href="/say-something-anything.html">یه چیزی بگو، هر چی!</a></li>
        
          <li><a href="/how-to-love-myself.html">خودتو دوست داشته باش ینی چی؟</a></li>
        
          <li><a href="/driving-rules.html">هنگام رانندگی به جای نایس بودن، قابل پیش بینی باشیم</a></li>
        
        </ul>
    </div>
  

  <small id="post-tags">
    
      <i class="fas fa-tag"></i>
      <a rel="tag" href="nlp">nlp</a>
    
      <i class="fas fa-tag"></i>
      <a rel="tag" href="اموزش">اموزش</a>
    

    <i class="fas fa-code"></i>
    
    
    <a href="https://raw.githubusercontent.com/spacelover1/personalBlog/master/_posts/2021-07-10-nlp-basics-one.md">سورس</a>
    
  </small>

  <nav class="pagination">
    
      <a href="/nlp-basics-two.html" class="pagination--pager" title="مبانی پردازش زبان طبیعی(NLP)- دو">قبلی</a>
    

    
      <a href="/why-are-we-here.html" class="pagination--pager" title="چرا اینجا هستیم؟">بعدی </a>
    
  </nav>

  <section>
    
      
<section id="static-comments">
  
  
  

  <form id="comment-form" name="comment" netlify>
    <input name="page_id" style="display:none" value="/nlp-basics-one">
    <input id="reply-to" name="reply-to" style="display:none">
    <label for="message">دیدگاه<sup class="required">*</sup> &nbsp;<small>می‌توانید با <a href="http://commonmark.org/help/" target="_">مارک‌داون</a> هم بنویسید.</small><br><small id="replyToVisualClue"></small>
      <textarea id="message" name="message" required alt="no!!" onkeyup="preview()"></textarea>
      <div id="preview"></div>
    </label>
    <label for="name">نام<sup class="required">*</sup>
      <input id="name" type="text" name="name" required>
    </label>
    <label for="email">ایمیل<sup class="required">*</sup>
      <input id="email" type="email" name="email" required>
    </label>
    <label for="website">وبسایت
      <input id="website" type="url" name="website">
    </label>
    <div data-netlify-recaptcha></div>
    <div style="text-align:left">
      <button type="submit" class="button">ارسال</button>
    </div>
  </form>

</section>
<script src="https://spacelover.ir/assets/js/showdown.min.js" type="text/javascript">
</script>
<script type="text/javascript">
  function preview() {
    var converter = new showdown.Converter();
    var markdown = document.getElementById("message").value;
    document.getElementById("preview").innerHTML = converter.makeHtml(markdown);
  }

  function replyTo(commentID) {
    var comment = document.getElementById(commentID);
    document.getElementById("reply-to").value = comment.querySelector("span[commentId]").innerText;
    document.getElementById("replyToVisualClue").innerText = "[در جواب " + comment.querySelector("span[commenter]").innerText + "]";
  }
</script>
    
  </section>

</article>
</div>
    </main><footer>

  <div class="wrapper">

    

    <p id="footer-description">هر چیزی به وقتش اتفاق میفته:)
</p>

    <ul class="social-media-list">
      
      <li>
        <i class="fab fa-github"></i>
        <a href="https://github.com/spacelover1">
          <span class="username social-media-text">Github</span>
        </a>
      </li>
      
      
      
      
        <li>
          <i class="fas fa-envelope-open"></i>
          <a href="mailto:spacelover1@gmail.com">
            <span class="username social-media-text">Email</span></a></li>
      
    </ul>

  <div id="footer-extra">
    <small id="license">
  <i class="fab fa-creative-commons"></i>
  مطالب این وبلاگ تحت مجوز
  <a rel="license" href="http://creativecommons.org/licenses/by/4.0/deed.fa">
    کریتیو کامنز اتریبیوشن ۴.۰ اینترنشنال
  </a>قرار دارد.
</small>
    <a id="atom" href="https://validator.w3.org/feed/check.cgi?url=https://spacelover.ir/feed.xml"><img src=https://spacelover.ir/assets/img/valid-atom.png alt="[Valid Atom 1.0]" title="Validate my Atom 1.0 feed" /></a>
  </div>

  </div>

  <small id="build-time">Site generated on Sun, 08 May 2022 02:24:25 +0430</small>

</footer>
<script src="https://spacelover.ir/assets/js/main.js"></script>
    
  </body>
</html>