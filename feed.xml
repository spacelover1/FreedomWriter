<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
<title type="text" xml:lang="fa">نویسنده آزاد</title>
<link rel="alternate" type="text/html" href="https://spacelover.ir"/>
<link rel="self" type="application/atom+xml" href="https://spacelover.ir/feed.xml"/>
<updated>2022-05-08T00:03:38Z</updated>
<id>urn:uuid:1e7d9dd8-29e4-475d-adc5-17494fb80445</id>
<author>
  <name>نویسنده</name>
  <uri>https://spacelover.ir</uri>
  <email>spacelover1@gmail.com</email>
</author>
<rights>Commons Attribution 4.0 International</rights>

  <entry>
    <title>یه چیزی بگو، هر چی!</title>
    <link rel="alternate" type="text/html" href="https://spacelover.ir/say-something-anything.html"/>
    <id>urn:uuid:</id>
    <updated>2022-04-04T00:00:00Z</updated>
    
    <content type="html">
      <![CDATA[<div dir="rtl"><p>مدت زیادیه که پستی ننوشتم، بخشیش بخاطر مشغله‌های زندگی و بخشیش هم بدلیل تنبلی!</p>

<p>این مدت یک شرکت عوض کردم در کل سال 1400 در سه تا شرکت مختلف بودم که الان تو سومی‌شم. یک پست درباره سوابق کاری و مصاحبه‌ها و این‌ مسائل می‌نویسم حتما.</p>

</div>]]>
    </content>
  </entry>

  <entry>
    <title>خودتو دوست داشته باش ینی چی؟</title>
    <link rel="alternate" type="text/html" href="https://spacelover.ir/how-to-love-myself.html"/>
    <id>urn:uuid:</id>
    <updated>2021-12-31T00:00:00Z</updated>
    <category term="خودشناسی"/>
    <content type="html">
      <![CDATA[<div dir="rtl"><p>چند روز پیش سر یه کلاسی بودم که استاد به یکی از بچه‌ها گفت برای اینکه کسی تو رو دوست داشته باشه اول از همه باید خودت خودتو دوست داشته باشی. دوست داشته شدن لیاقت می‌خواد. این حرف رو قبلا هم شنیده بودم. با خودم فک می‌کردم چطوری خودمو دوست داشته باشم. یکم درباره‌ش خوندم و برای اینکه یادم بمونه و تمرین کنم می‌خوام اینجا هم بنویسم درباره‌ش.<br />
سایتی که در این پست به عنوان <a href="https://www.healthline.com/health/13-self-love-habits-every-woman-needs-to-have">منبع</a> ازش استفاده می‌کنم ضمایر مخاطب استفاده کرده ولی من ضمایر رو به خودم برگردوندم. شما هم اگر دوست داشتین این موارد رو امتحان کنید.</p>

<h2 id="خودم-رو-با-کسی-مقایسه-نمیکنم">خودم رو با کسی مقایسه نمی‌کنم</h2>

<p>اینکه فلان شخص در فلان شرکت کار می‌کنه هیچ ارتباطی به من نداره. هر شخصی زندگی خودشو داره از اول و این باعث می‌شه که هر کدوممون تک و منجصر به فرد باشیم. بنابراین مقایسه بخشی از زندگی افراد دیگه با خودمون کاملا بی معنیه. من انرژی و تمرکزمو میذارم روی بهتر شدن خودم و انجام کارایی که دوست دارم.</p>

<h2 id="نگران-افکار-بقیه-نیستم">نگران افکار بقیه نیستم</h2>

<p>اینکه بقیه چه فکری درباره من می‌کنن مهم نیست. من نباید بقیه رو راضی نگه دارم. انرژی من باید صرف شاد کردن خودم بشه.</p>

<h2 id="به-خودم-اجازه-اشتباه-کردن-میدم">به خودم اجازه اشتباه کردن می‌دم</h2>

<p>اکثر کارایی که الان توشون قوی شدم بخاطر اشتباهاتی بوده که قبلا انجام دادم. وقتی یه کاری رو تا حالا انجام ندادیم، احتمال زیاد اولین بار اشتباه می‌کنیم و بخاطر اون اشتباه روش درست رو یاد می‌گیریم و خیلی چیزای دیگه. <br />
پس دست به کار می‌شم و از اشتباه نمی‌ترسم.</p>

<h2 id="ظاهر-من-ارزش-من-رو-تعیین-نمیکنه">ظاهر من، ارزش من رو تعیین نمی‌کنه</h2>

<p>من لباسی رو می‌پوشم که توش حس خوبی دارم.</p>

<h2 id="افراد-سمی-رو-از-زندگیم-حذف-میکنم">افراد سمی رو از زندگیم حذف می‌کنم</h2>

<p>از آدمایی که حس خوبی ازشون نمی‌گیرم و انرژی بیهوده ازم می‌گیرن فاصله می‌گیرم تا جا برای اونایی که ارزش دوستی با من رو دارن باز بشه.</p>

<h2 id="ترسهام-رو-پردازش-میکنم">ترس‌هام رو پردازش می‌کنم</h2>

<p>بررسی احساس ترس و اضطراب باعث می‌شه ریشه مشکلات پیدا شهتا بتونیم حلشون کنیم.</p>

<h2 id="میدونم-که-بهترین-تصمیمات-رو-میگیرم">می‌دونم که بهترین تصمیمات رو می‌گیرم</h2>

<p>هر کسی خودش، خودش رو بهتر می‌شناسه و می‌دونه چی حالشو خوب می‌کنه.<br />
یه قانونی که برای خودم دارم اینه که اگر جوابم در انجام کاری “نمی‌دونم” بود انجامش نمی‌دم تا زمانی که به اطمینان برسم.</p>

<h2 id="از-هر-فرصتی-که-پیش-میآد-استفاده-میکنم-یا-خودم-فرصت-میسازم">از هر فرصتی که پیش می‌آد استفاده می‌کنم یا خودم فرصت می‌سازم</h2>

<p>اگر نگران فکر بقیه نباشیم و از اشتباه نترسیم می‌تونیم از این فرصت‌ها استفاده کنیم.
من اگر دوست داشته باشم با کسی صحبت کنم باهاش تماس می‌گیرم و منتظر اون نمی‌شم.
یا اگر دوست داشته باشم تو یه جمع جدیدی باشم، باهاشون صحبت می‌کنم و اگر دوست داشتن اونوقت دوستای جدید پیدا می‌کنیم :)</p>

<h2 id="خودم-رو-در-اولویت-میذارم">خودم رو در اولویت می‌ذارم</h2>

<p>علائق خودم رو (به موقعش) در اولویت می‌ذارم و برای خودم ارزش قائل هستم.</p>

<h2 id="درد-و-شادی-رو-حس-میکنم">درد و شادی رو حس می‌کنم</h2>

<p>به احساسات مختلف اجازه بروز می‌دم و درکشون می‌کنم.
یه موردی رو هم بگم: هر زمانی احساس افسردگی داشتین ورزش کنید و ساعت خوابتونو تنظیم کنید. صبح زود بیدار شید و شب به موقع بخوابید.
هر زمانی هم احساس شیدایی داشتین یعنی خوشحالی و انرژی زیاد سعی کنید بخوابید و کمتر تحرک داشته باشید.</p>

<h2 id="نظراتم-رو-در-جمع-بیان-میکنم">نظراتم رو در جمع بیان می‌کنم</h2>

<p>نظر هر شخصی محترم و مهمه همینطور نظر من. وقتی یه سوالی مطرح می‌شه نظرم رو بیان می‌کنم بدون ترس از قضاوت. مهم نیس بقیه چه فکری می‌کنن. یه نکته‌ای که وجود داره اینه که اصولا بقیه فکری درباره شما نمی‌کنن. شما خودت فک کن چقدر به بقیه فک می‌کنی؟ خیلی کم. ادما بیشتر متمرکز هستن روی اعمال خودشون.</p>

<h2 id="زیبایی-رو-در-چیزای-کوچیک-ببینیم">زیبایی رو در چیزای کوچیک ببینیم</h2>

<p>قدردان زیبایی‌ها و چیزایی که حس خوبی بهم می‌ده هستم و این باعث می‌شه بیشتر زیبایی ببینم و جذب کنم.</p>

<h2 id="با-خودم-مهربونم">با خودم مهربونم</h2>

<p>همینطوری در طول روز همه دارن از ادم انتقاد می‌کنن و به ادم حرف می‌زنن. من خودم با خودم مهربون هستم و از کلمات خوب برای توصیف خودم استفاده می‌کنم.
من تا الان که زندگی کردم خیلی کارای باارزش و قشنگ انجام دادم و از این به بعد هم به ارزش افرینی‌ها ادامه می‌دم. برای همین ذهن زیبا به خودم افتخار می‌کنم و از خودم ممنونم.</p>

</div>]]>
    </content>
  </entry>

  <entry>
    <title>هنگام رانندگی به جای نایس بودن، قابل پیش بینی باشیم</title>
    <link rel="alternate" type="text/html" href="https://spacelover.ir/driving-rules.html"/>
    <id>urn:uuid:</id>
    <updated>2021-12-30T00:00:00Z</updated>
    <category term="رانندگی"/>
    <content type="html">
      <![CDATA[<div dir="rtl"><p>در ردیت یک سابی وجود داره با عنوان <a href="https://www.reddit.com/r/IdiotsInCars/">IdiotsInCars</a> در اینجا ویدیوهای متفاوتی (معمولا در امریکا) از اشتباهات رانندگی به اشتراک گذاشته می شه. بعضیاش خنده دار و فانه، یه سری حماقت های رانندگی، یه سری هم کارهای خطرناک. یه جمله ای که خیلی اوقات تو کامنتا تکرار میشه اینه: be predictable not nice. مثلا وقتی یکی از فرعی می‌خواد وارد اصلی بشه باید صبر کنه و توقف کامل داشته باشه اگر خیابون خالی بود و راه داشت می‌تونه بپیچه. شخصی هم که داره در خیابان اصلی رانندگی می‌کنه نباید بخاطر ماشینی که از فرعی میاد توقف کنه چون ممکنه باعث تصادف بشه. یک مورد دیگه‌ای که متوجه شدم اینکه در اکثر ایالت‌ها کسی که از پشت بزنه به ماشین جلویی حتما مقصر نیست و مقصر راننده‌ایه که توقف داشته در مسیری که نباید. برعکس ایران که هر ماشینی از پشت بزنه بهت مقصره.</p>

<h2 id="خطهای-سفید-در-خیابانها-بیدلیل-نیست">خط‌های سفید در خیابان‌ها بی‌دلیل نیست</h2>

<p>یک موردی که خیلی برام آزاردهنده است موقع رانندگی تو تهران اینه که اتوبان سه لاینه طراحی شده، ولی شما 5 لاین ماشین می‌بینید! ماشین‌هایی که در لاین‌های کناری هستن طوری رانندگی می‌کنن که یک سانت با گاردریل فاصله دارن تا جا باز بشه برای ماشینای دیگه که بیان روی خط‌ها رانندگی کنن!!!! واقعا این چه منطق و دلیلی داره؟ چرا اینطوری رانندگی می‌کنند من متوجه نمی‌شم. این کار هیچ جوره قابل توجیه نیست. ترافیکه؟ خب برای همه است شما اگر با نظم رانندگی کنی هم تصادف کمتر می‌شه هم زودتر می‌رسی. خط‌ها استاندارد نیست؟ خیر کاملا استاندارده، قرار نیست خط‌ها فیت ماشین شما باشه تا ماشین‌ها در لاین‌ها به هم بچسبن. <br />
کاری که من انجام می‌دم اینه که در یک لاین طوری رانندگی می‌کنم که باید. یعنی نمی‌رم بچسبم به ماشین بغلی تا لاین باز شه. تو لاین خودم حرکت می‌کنم و اینطوری ماشین‌های پشتی هم حداقل تو لاین من مرتب می‌شن. امیدوارم تعداد بیشتری افراد به این خط‌ها توجه کنند.</p>

<h2 id="از-چراغهای-راهنمای-خودرو-استفاده-کنیم">از چراغ‌های راهنمای خودرو استفاده کنیم!</h2>

<h2 id="در-رانندگی-مثل-بقیه-کارها-نگاه-جنسیت-زده-نداشته-باشیم-حتی-شما-خانم-محترم">در رانندگی (مثل بقیه کارها) نگاه جنسیت زده نداشته باشیم. حتی شما خانم محترم!</h2>
</div>]]>
    </content>
  </entry>

  <entry>
    <title>معرفی یک آهنگ، یک کتاب و یک سایت</title>
    <link rel="alternate" type="text/html" href="https://spacelover.ir/suggestions.html"/>
    <id>urn:uuid:</id>
    <updated>2021-12-10T00:00:00Z</updated>
    <category term="معرفی"/><category term="کتاب"/><category term="اهنگ"/><category term="سایت"/>
    <content type="html">
      <![CDATA[<div dir="rtl"><p>زمانی که کرونا گرفتم به حدی ضعیف شده بودم که فکر می‌کردم دیگه هیچ‌وقت خوب نمی‌شم، اما تو این مدت حالم بهتر شد و تونستم دوباره برم طناب بزنم و با بچه‌ها فوتبال بازی کنم و دیشب هم نزدیک یک ساعت دوچرخه‌سواری کردم. در این پست یک آهنگ یک کتاب و یک وبسایت معرفی شده، احتمالا این شیوه رو به کار ببرم و همچین پستی رو باز هم داشته باشیم. به نظرم ایده خوبیه :)</p>

<h2 id="آهنگ-enemy">آهنگ Enemy</h2>

<p>در این پست می‌خوام یکی از آهنگ‌های گروه Imagine Dragons رو معرفی کنم. آهنگ جدیده و محتواش عالی. یه جورایی همه این رو حس کردن. <a href="https://www.youtube.com/watch?v=D9G1VOjN_84"><u>لینک آهنگ Enemy</u></a> و این هم <a href="https://www.youtube.com/watch?v=4TKDGCBbD2s"><u>لینک این آهنگ در یکی از کنسرت‌ها</u></a>. خیلی خوووبه. شرکت در این کنسرت‌ها را برای همه آرزو می‌کنم :)</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Tell you you're the greatest
But once you turn they hate us
</code></pre></div></div>

<p>هر فرد بنا بر تجربیاتش، برداشت خودش رو از هر آهنگ داره. برداشت من از این آهنگ اینه که اکثر آدما تا وقتی باهاتن و دوست دارن که طبق سلیقه اون‌ها عمل کنی. زمانی که خلاف میلشون باشه ازت بدشون میاد. (not that anyone cares, but people love to be loved)</p>

<p>دوست دارم برداشت‌های شما رو هم بدونم.</p>

<h2 id="کتاب-emotional-blackmail">کتاب Emotional Blackmail</h2>

<p>خوندن <a href="https://www.amazon.com/Emotional-Blackmail-People-Obligation-Manipulate/dp/0060928972">این کتاب</a> به همه توصیه می‌شه. تا الان نصفش رو خوندم. تا اینجایی که خوندم کتاب شامل یکسری داستان از مراجعه کننده‌های خانم فوروارده که با هدف گرفتن نقاط ضعف طرف مقابل که معمولا هم افراد نزدیک به انها هستند را مجبور به انجام کاری می‌کنند. &lt;/br&gt;
شاید این حس را تجربه کرده باشید که وقتی فرد نزدیکی از شما چیزی می‌خواهد که خلاف میل شماست یا آن کار را انجام می‌دهید که او را خشنود نگه دارید و یا زمانی که انجامش نمی‌دهید حس خیلی بدی دارید و با خود می‌گویید کاش چیزی که می‌خواست را براورده نمی‌کردم. اگر اینطوره حتما این کتاب رو بخونید.</p>

<h2 id="وبسایت-ludwig">وبسایت Ludwig</h2>

<p>بعضی وقتا هنگام نوشتن متن انگلیسی ممکنه یک ترکیبی استفاده کنید که مطمئن نباشید درسته و استفاده می‌شه یا نه. می‌تونید اون ترکیب چند کلمه‌ای رو به این وبسایت بدید و <a href="https://ludwig.guru/"><u>این وبسایت</u></a> با جستجوش در منابع مختلف انگلیسی زبان مثل New York Times یک سری جملات با این ترکیب برمی‌گردونه و شما نحوه استفاده از اون کلمات رو در جملات یاد می‌گیرید.</p>

</div>]]>
    </content>
  </entry>

  <entry>
    <title>کرونا</title>
    <link rel="alternate" type="text/html" href="https://spacelover.ir/covid-19.html"/>
    <id>urn:uuid:</id>
    <updated>2021-09-01T00:00:00Z</updated>
    <category term="زندگی"/>
    <content type="html">
      <![CDATA[<div dir="rtl"><p>تقریبا دو هفته پیش، شب قبل از خواب لرز گرفتم فرداش وقتی بیدار شدم تب و لرز داشتم. <br />
تا سه چهار شب تب و لرز داشتم و کم کم بدنم ضعیف شد یعنی توانی نداشتم دیگه. انگار یه چیزی داره تمام ویتامینای بدن رو نابود می‌کنه و هر چی انرژیه از بدن می‌گیره. <br />
انقدر بدنم ضعیف شده بود و احساس ناتوانی داشتم که تا همین دو روز پیش حس می‌کردم دیگه هیچ‌وقت مثل قبل نمی‌شم و همیشه ضعیف می‌مونم. حس بدیه. بعد از یک هفته حس چشایی و بویاییم رو از دست دادم. حس بویایی به کل از بین رفته ولی مزه‌های اصلی رو متوجه می‌شم، مثل شور و شیرین و تند. اما مزه غذا رو به صورت کلی و مثل قبل نمی‌فهمم.
خوشبختانه ریه‌های من درگیر نشد ولی خب مشکلات دیگه بوجود آورد، مثل درد عضلات و ماهیچه‌ها.
سیستم گوارش هم تا حدودی مختل کرده.</p>

<p>این مدت کار زیادی انجام ندادم و از خیلی کارهام هم عقب افتادم و حتی انگیزه ام رو از دست دادم. بیشتر با اینترنت و فیلم و آهنگ خودم رو سرگرم کردم.</p>

<p>مینی سریال Queen’s Gambit رو دیدم. خوب بود ولی خیلی دوست نداشتم.<br />
بعضی قسمت های ریک و مورتی  و سریال کامیونیتی رو دوباره دیدم.</p>

<p>با Skeeter Davis آشنا شدم، یه خواننده قدیمی آمریکایی که سبک آهنگاش کانتریه و من خیلی ازش خوشم اومد. یکی از معروف ترین آهنگاش <a href="https://www.youtube.com/watch?v=xHa6a3FtPJg">End of the World</a>. خیلی از آهنگاش خوشم میاد. یکی دیگه از آهنگای قشنگش I want go where nobody knows me. حسی که الان دارم و چیزی که الان می خوام.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>I want to go where no one knows me
Where I can start my life anew

I want to go where no one knows me
My soul is sick my heart is sore
I want to go where all are strangers
I don't believe in friends no more
</code></pre></div></div>

<p>این روزا هم شطرنج بازی می کنم و تو یوتوب و ردیت می چرخم. اگر روزا رو همینطوری سپری کنم خروجی نهایی همین وضعیت الانه:/ این پست رو با یک نقل قول تموم می کنم:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>"The strong do what they can and the weak suffer what they must."
</code></pre></div></div>

</div>]]>
    </content>
  </entry>

  <entry>
    <title>مبانی پردازش زبان طبیعی(NLP)- چهار</title>
    <link rel="alternate" type="text/html" href="https://spacelover.ir/nlp-basics-four.html"/>
    <id>urn:uuid:</id>
    <updated>2021-07-19T00:00:00Z</updated>
    <category term="nlp"/><category term="اموزش"/>
    <content type="html">
      <![CDATA[<div dir="rtl"><h2 id="مهندسی-ویژگی-feature-engineering"><strong>مهندسی ویژگی (Feature Engineering)</strong></h2>

<p>خلاصه ای از جلسات قبل: تا الان یاد گرفتیم که چطوری دیتایی که یکم نامنظمه رو بخونیم. و با حذف علائم نگارشی و کلمات توقف و جدا کردن کلمات و همچنین استفاده از استمر دیتا رو تمیز کردیم. و در آخر برداری کردن دیتا رو با چند روش مختلف برای ساخت مدل یاد گرفتیم. پس ما الان یک دیتا و لیبل زده تمیز داریم که برای استفاده در مدل آماده است.<br />
حالا یک قدم تا ایجاد مدل واقعی فاصله داریم و اون مهندسی ویژگیه.</p>

<h3 id="مهندسی-ویژگی-چیه">مهندسی ویژگی چیه؟</h3>

<p>مهندسی ویژگی یعنی یک سری ویژگی جدید بسازیم یا از ویژگی های موجود رو طوری تغییر بدیم تا بیشترین بهره وری رو از دیتا داشته باشیم.<br />
الان که دیتا رو برداری کردیم با توجه به روشی که استفاده کردیم از یه سری ویژگی های محدود استفاده می کنه. در اینجا ویژگی هایی که می تونیم اضافه کنیم مثلا می تونه موارد زیر باشه:</p>

<ul>
  <li>طول پیام. شاید مثلا پیام های اسپم طولانی تر باشند.<br /></li>
  <li>درصد علائم نگارشی استفاده شده در پیام. شاید در پیام های واقعی خیلی از علئم نگارشی استفاده نشه.<br /></li>
  <li>تعداد کارکترهای با حروف بزرگ. چوت این دیتاست انگلیسیه می تونیم همچین ویژگی ای داشته باشیم.<br /></li>
</ul>

<p>این چند نمونه ویژگی ایه که در این دیتاست می تونیم برای تشخیص بهتر پیام های اسپم و غیراسپم استفاده کنیم.</p>

<p>و برای تغییر شکل  (transform) ویژگی های موجود در دیتا یکسری کارها و فرمول های رایج وجود داره، مثلا:</p>

<ul>
  <li>تغییرات توانی (Power transformation): مثل محاسبه جذر یا توان دو دیتا و …<br /></li>
  <li>استانداردسازی دیتا. بعضی مدل ها زمانی بهتر کار می کنند که تمام ویژگی هاشون در یک مقیاس (scale) باشه.</li>
</ul>

<p>برای نمونه دیگری از تبدیل به مثال زیر دقت کنید:<br />
تصویر سمت چپ یک نمونه دیتاست رو نشون می ده که داده ها پراکنده هستند و نمی شه ارتباط درستی پیدا کرد. در همچین مواردی که یک دنباله طولانی داریم از لگاریتم استفاده می کنیم.</p>

<div style="text-align:center"><img src="https://raw.githubusercontent.com/spacelover1/NLP-with-Python/main/4-FeatureEngineering/FE_transformation.PNG?token=AEGZAVWA2CDPI5ZWFAG3HVTA6V6WA" /></div>

<p>در حالت کلی برای ایجاد ویژگی باید مسئله رو به درستی درک کنیم و دید خوبی نسبت بهش پیدا کنیم، و همچنین باید خلاقیت داشته باشیم و تو ذهنمون تصور کنیم که از چی می خوایم به چی برسیم و برای رسیدن به اون هدف چه ویژگی هایی نیاز داریم. مثلا در تشخیص پیام اسپم و غیراسپم پیدا کردن تعداد حروف a در پیام ها ممکنه کمک چندانی به حل مسئله نکنه و ویژگی مناسبی برای این مسئله نباشه ولی مثلا تعداد علائم نگارشی استفاده شده یا طول پیام به نظر مفیدتر می آد.</p>

<h2 id="تولید-ویژگی">تولید ویژگی</h2>

<p>بریم سراغ کد: <br />
اینجا می خوایم دو تا ویژگی طول پیام و درصد علائم نگارشی در پیام رو ایجاد کنیم.</p>

<p>بعد از خوندن دیتای خام می آییم ویژگی طول پیام رو اول می سازیم:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>datsset['body_len'] = dataset['body'].apply(lambda x: len(x) - x.count(" "))
</code></pre></div></div>

<p>خب <code class="language-plaintext highlighter-rouge">len(x)</code> به ما طول پیام رو می ده ولی نکته ای که هست اینه که کاراکتر فاصله هم شمرده می شه. مثلا ممکنه یک پیام به طول 10، نه کاراکتر فاصله داشته باشه و این نباید برابر باشه با پیامی که ده کاراکتر غیرفاصله داره. پس برای همین تعداد فاصله ها شمرده می شه و از طول کل پیام کم می شه.</p>

<p>یک ویژگی مفید دیگه هم درصد علائم نگارشی در پیام هاست. برای محاسبه ش باید یک تابع بنویسیم:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def count_punctuation(text):
    count = sum([1 for char in text if char in string.punctuation])
    return round(count/(len(char) - char.count(" ")), 3) * 100
</code></pre></div></div>

<p>این تابع چیکار می کنه؟ اول از همه یکی یکی علائم نگارشی رو می شماره و در نهایت با تابع <code class="language-plaintext highlighter-rouge">sum()</code> این یک ها رو جمع می کنیم. قراره درصد علائم نگارشی استفاده شده در پیام رو برگردونه. مقدار <code class="language-plaintext highlighter-rouge">count</code> تعداد علائم نگارشی یک پیامه. برای محاسبه درصد باید بیاییم این مقدار رو تقسیم بر کل کاراکترهای غیرفاصله پیام کنیم. بعد چون یک مقدار اعشاری برمی گردونه برای اینکه عدد خیلی طولانی نباشه رند می کنیم عدد رو تا سه رقم اعشار نشون بده و در نهایت در 100 ضرب می کنیم تا از حالت اعشار خارج شه.</p>

<p>کد کامل رو <a href="https://github.com/spacelover1/NLP-with-Python/blob/main/4-FeatureEngineering/FeatureCreation.ipynb">اینجا</a> می تونید مشاهده کنید.</p>

<h2 id="ارزیابی-ویژگی">ارزیابی ویژگی</h2>

<p>حالا باید بررسی کنیم که این ویژگی ها برای این دیتاست مناسبن یا نه؟ که آیا می تونیم برای استخراج اطلاعات بهتر ازشون استفاده کنیم یا نه؟ از کتابخونه matplotlib برای رسم نمودار و هیستوگرام استفاده می کنیم. از تابع <code class="language-plaintext highlighter-rouge">hist()</code> در مجموعه توابع <code class="language-plaintext highlighter-rouge">pyplot</code> استفاده می کنیم:</p>

<p>ویژگی اولی که ساختیم طول پیام بود. حدس زدیم که احتمالا پیام های اسپم طولانی ترند. بریم ببینیم این حدس درسته یا نه؟</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>bins = np.linspace(0, 200, 40)
pyplot.hist(dataset[dataset['label'] == 'spam']['body_len'], bins, alpha=0.5, density=True, label='spam')
</code></pre></div></div>

<p>محدوده و تعداد استوانه ها رو با <code class="language-plaintext highlighter-rouge">bin</code> مشخص می کنیم و مقادیری که می بینید به این ترتیبه: <code class="language-plaintext highlighter-rouge">bins = np.linspace(min_boundary, max_boundary, n_bins)</code>.<br />
ممکنه در کدهای قدیمی تر پارامتر <code class="language-plaintext highlighter-rouge">normed=True</code> رو ببینید که این معادل پارامتر <code class="language-plaintext highlighter-rouge">density</code> است که اینجا استفاده کردیم در نسخه های جدیدتر پایتون از این پارامتر استفاده می شه.</p>

<p>همین خط کد رو باید یرای پیام های غیراسپم هم بنویسیم تا بتونیم مقایسه کنیم.<br />
بعد از گرفتن خروجی می بینیم که پیش بینی مون درست بوده و پیام های اسپم بسیار طولانی تر از پیام های غیراسپمن. پس این ویژگی که ایجاد کردیم مناسب و مفیده.</p>

<p>برای تست ویژگی بعدی همین خط کد رو داریم فقط به جای <code class="language-plaintext highlighter-rouge">[body_len]</code> باید <code class="language-plaintext highlighter-rouge">[punct%]</code> رو بذاریم تا ببینیم طبق حدسمون پیام های اسپم بیشتر از غیراسپما علائم نگارشی دارند؟</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>bins = np.linspace(0, 50, 40)
pyplot.hist(dataset[dataset['label'] == 'spam']['punct%'], bins, alpha=0.5, density=True, label='spam')
</code></pre></div></div>

<p>و بعد از اجرای کد می بینیم که تفاوت چشمگیری در استفاده از علائم نگارشی بین پیام های اسپم و غیراسپم وجود نداره. و همونطور که در این توزیع دیده می شه یه دنباله ای در پیام های غیراسپم ایجاد شده که احتمالا باید از تبدیل (transformation) استفاده کنیم تا بهتر بتونیم تصمیم بگیریم.</p>

<p><a href="https://github.com/spacelover1/NLP-with-Python/blob/main/4-FeatureEngineering/FeatureCreation%26Evaluation.ipynb">کد کامل این بخش</a>.</p>

<h2 id="تبدیل-transformation">تبدیل (Transformation)</h2>

<p>در این بخش می خوایم بررسی کنیم که دو تا ویژگی ای که ایجاد کردیم نیاز به تبدیل دارند یا نه.<br />
اولین کاری که باید انجام بدیم اینه که توزیع کاملشون رو رسم کنیم و بعد طبق اون تصمیم بگیریم. مواردی که نشون می ده تبدیل نیازه یا نه، عدم تقارن شدید، دنباله طولانی و outlierها (یعنی اون نقاطی که خیلی از توزیع اصلی دورافتادن).</p>

<h3 id="طول-پیام">طول پیام</h3>

<p>برای شروع <code class="language-plaintext highlighter-rouge">bins</code> رو مثل قبل تعریف می کنیم، از صفر شروع شه تا 200 بره و 40 تا bin تولید شه:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>bins = np.linspace(0, 200, 40)
pyplot.hist(dataset['body_len'], bins)
</code></pre></div></div>

<p>نیازی به پارامترای دیگه نیست چون می خوایم توزیع کلی طول پیام ها رو ببینیم، بدون توجه به لیبلشون.</p>

<div style="text-align:center"><img src="https://raw.githubusercontent.com/spacelover1/NLP-with-Python/main/4-FeatureEngineering/length_distribution.PNG" alt="length distribution" /></div>

<p>همونطور که قبلا دیدیم طول پیام های اسپم بیشتر از غیراسپم ها بود پس این توزیع درست و با معنیه. پس این ویژگی نیازی به تبدیل نداره.</p>

<h3 id="درصد-پیام-های-نگارشی">درصد پیام های نگارشی</h3>

<p>برای بررسی ویژگی بعد 200 رو به 50 تغییر می دیم یعنی متن های تا 50 تا علائم نگارشی رو بررسی کنه.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>bins = np.linspace(0, 50, 40)
pyplot.hist(dataset['punct%'], bins)
</code></pre></div></div>

<div style="text-align:center"><img src="https://raw.githubusercontent.com/spacelover1/NLP-with-Python/main/4-FeatureEngineering/punct_percentage.PNG" alt="punctuation percentage" /></div>

<p>این توزیع رو همونطور که می بینید تقارن نداره مقدار زیادی از دیتا نزدیک صفر جمع شده و همینطور یک دنباله طول و دراز هم تشکیل شده که نشون می ده نیاز به تبدیل داره.</p>

<p>حالا که ویژگی هایی که نیاز به تبدیل دارند رو مشخص کردیم، باید تبدیل رو شروع کنیم.</p>

<p>تبدیل (Transformation) فرایندیه که هر داده رو در یک ستون مشخص به صورت سیستماتیک تغییر می ده (مثلا محاسبه جذر یا توان دوم هر داده) تا دیتا رو برای استفاده بهتر مدل از اون، پاکسازی کنه.</p>

<p>مجموعه تبدیلی که اینجا استفاده می کنیم بسیار رایجه و Box-Cox Power Transformations نام داره. فرم پایه این تبدیلات y به توان x است. جدول زیر این تبدیل رو برای بازه <code class="language-plaintext highlighter-rouge">[-2,2]</code> نمایش می ده:</p>

<div style="text-align:center"><img src="https://raw.githubusercontent.com/spacelover1/NLP-with-Python/main/4-FeatureEngineering/cox-box-transformation.PNG" alt="cox-box transformation" /></div>

<p>حالا اگر فرض کنیم 50 درصد یک متن علائم نگارشیه، در جدول بالا <code class="language-plaintext highlighter-rouge">x = 50</code> می شه.</p>

<h3 id="فرایند-تبدیل">فرایند تبدیل</h3>

<p>1- مشخص کردن بازه توانی <br />
2- اعمال هر تبدیل را به هر مقدار ویژگی انتخاب شده<br />
3- استفاده از معیارهایی برای تشخیص تبدیلی که بهترین توزیع را تولید می کند</p>

<p>بعد از بررسی ویژگی هایی که ایجاد کردیم دیدیم که ویژگی علائم نگارشی نیاز به تبدیل داره. کد اعمال تبدیل رو به این صورت می نویسیم:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>for i in range(1, 6):
    pyplot.hist((dataset['punct%'])** (1/i), bins=40)
    pyplot.title('transformation: 1/{}'.format(str(i)))
</code></pre></div></div>

<p><a href="https://github.com/spacelover1/NLP-with-Python/blob/main/4-FeatureEngineering/featureEngineering_transformation.ipynb">کد کامل این بخش</a></p>

<p>برای مطالعه بیشتر درباره تبدیل و مهندسی ویژگی <a href="https://towardsdatascience.com/data-transformation-and-feature-engineering-e3c7dfbb4899">این مقاله</a> رو می تونید مطالعه کنید.</p>

<p><a href="https://github.com/spacelover1/personalBlog/blob/master/ZeinabSalimi-cv.pdf">فایل</a></p>

</div>]]>
    </content>
  </entry>

  <entry>
    <title>مبانی پردازش زبان طبیعی(NLP)- سه</title>
    <link rel="alternate" type="text/html" href="https://spacelover.ir/nlp-basics-three.html"/>
    <id>urn:uuid:</id>
    <updated>2021-07-15T00:00:00Z</updated>
    <category term="nlp"/><category term="اموزش"/>
    <content type="html">
      <![CDATA[<div dir="rtl"><p>در این قسمت در مورد برداری کردن دیتا صحبت می کنیم. <br />
تا اینجا دیتا رو خوندیم و تا حدودی نرمالیزه کردیم. الان پایتون دیتا رو فقط یک سری رشته کاراکتر می بینه. حالا برای اینکه مدل ماشین لرنینگ و پایتون این دیتا رو درک کنه باید دیتا برداری بشه. برداری کردن یعنی چی؟ یعنی متن به عددصحیح تبدیل شه و یک بردار ویژگی ساخته شه.<br />
حالا بردار ویژگی در اینجا یعنی متن هر پیام رو بگیریم و به یک بردارعددی تبدیل کنیم که نمایش دهنده متن اون پیام باشه. <br />
چطوری این کار رو انجام می دیم؟ در ادامه درباره این مورد صحبت می کنیم.<br />
چندین روش برای برداری کردن ویژگی ها وجود داره که در ادامه سه روش رایج رو بررسی می کنیم.</p>

<h2 id="روش-اول-بردار-تعداد-count-vectorization"><strong>روش اول: بردار تعداد (Count Vectorization)</strong></h2>

<p>در این روش هر پیام گرفته می شه و هر کلمه به عنوان یک ویژگی در نظر گرفته می شه و بعد تعداد تکرار هر کلمه در اون پیام ثبت می شه. در نهایت یک ماتریسی داریم که هر سطر مربوط به یک پیام و هر ستون نمایش دهنده یک کلمه است. و در نهایت پایتون با بررسی این ماتریس یک ارتباطی بین کلمات موجود در پیام و لیبل اون پیام پیدا می کنه تا در آینده که بهش پیام های بدون لیبل بدیم بتونه به درسی برچسب گذاری کنه.</p>

<p>برای درک بهتر این فرایند به عکس زیر دقت کنید:</p>

<div style="text-align:center"><img src="https://raw.githubusercontent.com/spacelover1/NLP-with-Python/main/3-VectorizingRawData/vectorization_example.PNG" alt="vectorization_example" /></div>

<p>دراین تصویر فقط دو رشته offer و lol از لیست کلمات پیام ها انتخاب شده و تعداد تکرارشون محاسبه شده. همونطور که در جدول سمت چپ و راست می بینید پیام هایی که برچسب غیر اسپم دارند در آن ها رشته lol وجود داشته و تکرار شده ولی شامل رشته offer نیستند و برعکس پیام های اسپم اکثرا رشته offer رو شامل می شن. <br />
این یک مثال بسیار ساده برای درک فرایند و مفهوم بردار تعداد است.</p>

<p>حالا در عمل این روش رو پیاده می کنیم:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import pandas as pd
import re
import string
import nltk

pd.set_option('display.max_colwidth', 100)
dataset = pd.read_csv('SMSSpamCollection.tsv', sep='\t')
dataset.columns = ['label', 'body']

nltk.download('stopwords')
stopwords = nltk.corpus.stopwords.words('english')
ps = nltk.PorterStemmer()


def clean_text(text):
  text = "".join([word.lower() for word in text if word not in string.punctuation])
  tokens = re.split('\W+', text)
  text = [ps.stem(word) for word in tokens if word not in stopwords]
  return text
</code></pre></div></div>

<p>بعد از خوندن و پاکسازی دیتا، سراغ برداری کردن می ریم.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from sklearn.feature_extraction import CountVectorizer

count_vect = CountVectorizer(analyzer=func_name)
X_counts = count_vect.fit_transform(dataset['body'])
</code></pre></div></div>

<p>حالا می تونیم با استفاده از <code class="language-plaintext highlighter-rouge">X_counts.shape</code> تعداد پیام ها و تعداد رشته های منحصر بفرد در این پیام ها رو ببینیم. در این دیتاست 5567 پیام و 8104 رشته منحصر بفرد داریم که همون ویژگی های ما هستند. این اعداد تعداد سطرو و ستون های ماتریس رو نمایش می ده.<br />
و <code class="language-plaintext highlighter-rouge">count_vect.get_feature_names()</code> رشته های منحصربفرد رو نمایش می ده.</p>

<p>تابع هایپرپارامترهای دیگه ای هم داره که <a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html">اینجا</a> می تونید دربارشون بخونید.</p>

<p>حالا در اینجا برای یادگیری 20 پیام اول رو  برداری می کنیم:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sample = dataset[0:20]
count_vect_sample = CountVectorizer(analyzer=clean_text)
X_counts_sample = count_vect_sample.fit_transform(dataset['body'])
</code></pre></div></div>

<p>و الان وقتی سایز دیتای نمونه رو ببینیم 192 رشته منحصربفرد داریم. خروجی و کد کامل این بخش رو <a href="https://github.com/spacelover1/NLP-with-Python/blob/main/3-VectorizingRawData/CountVectorization.ipynb">اینجا</a> می تونید ببینید.</p>

<h2 id="روش-دوم-بردار-n-grams-n-gram-vectorizing"><strong>روش دوم: بردار N-Grams (N-gram vectorizing)</strong></h2>

<p>این روش هم تا حدود زیادی مشابه روش قبلیه و ساختار کدش مشابه اونه. در اینجا هم هر سطر پیام ها هستند ولی هر ستون به جای نمایش یک رشته، ترکیب nتایی از رشته هاست. برای درک بهتر تصویر زیر  رو ببینید:</p>

<div style="text-align:center"><img src="https://raw.githubusercontent.com/spacelover1/NLP-with-Python/main/3-VectorizingRawData/ngrams.png?token=AEGZAVTZYIIT2UNASADUQN3A6KJCE" alt="ngrams" /></div>

<p>مثل مرحله قبل دیتا رو می خونیم و بعد باید تابعی بنویسیم که مراحل پاکسازی رو انجام بده. بخش قبل یک لیستی از توکن رو می دادیم به vectorizer اما الان چون می خواد ترکیبی از کلمه ها رو بسازه باید ورودی بهش یک رشته بدیم. پس در آخر باید توکن ها رو مثل یک جمله کنار هم دیگه قرار بدیم و این کار رو با تابع <code class="language-plaintext highlighter-rouge">join()</code> انجام می دیم:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def clean_text(text):
  text = "".join([word.lower() for word in text if word not in string.punctuation])
  tokens = re.split('\W+', text)
  text = " ".join([ps.stem(word) for word in tokens if word not in stopwords])
  return text
</code></pre></div></div>

<p>اگر یادتون باشه خط اول کاراکترها رو یکی یکی بررسی می کرد برای پیدا کردن علائم نگارشی و در اخر با <code class="language-plaintext highlighter-rouge">join()</code> این کاراکترها رو بهم متصل کردیم. در <code class="language-plaintext highlighter-rouge">join()</code> دومی قرار کلمه ها کنار هم بیان تا جمله بسازن پس باید یک فاصله بین هر کلمه باشه.</p>

<p>در اینجا هم از CountVectorizer استفاده می کنیم:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ngram_vect = CountVectorizer(ngram_range=(2,2))
X_counts = ngram_vect.fit_transform(dataset['cleaned_text'])
</code></pre></div></div>

<p>در <code class="language-plaintext highlighter-rouge">ngram_range</code> مشخص می کنیم که ترکیب چندتایی از کلمه ها بسازه. مثلا (1,3) یعنی همه ترکیبهای یکی، دوتایی و سه تایی. <br />
همونطور که مشاهده می کنید تعداد فیچرها اینجا خیلی زیاد می شه. نکته ای که باید توجه کنیم اینه که چه زمانی از هر کدوم از این روش ها استفاده کنیم. با توجه به مسئله ممکنه یکی از این روش ها نتیجه بهتری بده. لزوما نمی شه گفت یکی از این روش ها بهتر از دیگری است.</p>

<p>کدهای این بخش در <a href="https://github.com/spacelover1/NLP-with-Python/blob/main/3-VectorizingRawData/NGrams.ipynb">اینجا</a> قابل مشاهده است.</p>

<h2 id="روش-سوم-te-idf-term-frequency--inverse-document-frequenct"><strong>روش سوم: TE-IDF (Term Frequency- Inverse Document Frequenct)</strong></h2>

<p>در این روش هم یک ماتریس ایجاد می شه که سطرها پیام ها هستند و هر ستون یک کلمه رو مشخص می کنه. اما سلول های این ماتریس دیگه تعداد تکرار کلمه رو نشون نمی ده بلکه وزن اون کلمه رو نشون می ده، تا اهمیت هر کلمه رو در اون پیام مشخص کنه. <br />
فرمول زیر برای محاسبه این وزنه:</p>

<div style="text-align:center"><img src="https://raw.githubusercontent.com/spacelover1/NLP-with-Python/main/3-VectorizingRawData/tf-idf.PNG?token=AEGZAVSASZLPTSDT75HLYL3A6PNMY" alt="tf-idf_formula" /></div>

<p>بریم ببینیم هر کدوم از این عبارات در فرمول چیو مشخص می کنه و چطوری محاسبه می شه:</p>

<p>عبارت tf تعداد تکرار یک کلمه در یک جمله تقسیم بر تعداد کل کلمات اون جمله. <br />
مثلا در جمله “امروز هوا گرم است” اگر کلمه “گرم” رو در نظر بگیریم، مقدار tf می شه: 1/4 یا 0.25</p>

<p>قسمت دوم این فرمول مشخص می کنه که هر کلمه چند بار تو کل جملات (پیام ها) تکرار شده.
در همین مثال اگر متن ما شامل 20 جمله باشه و کلمه گرم فقط یک بار تکرار شده باشه، نتیجه می شه:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>N = 20, df = 1 &gt;&gt;&gt;&gt; log(N/df) = log(20/1) = 1.301
</code></pre></div></div>

<p>مبنای لگاریتم هم 10 است.</p>

<p>و درنهایت:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0.25 * 1.301 = 0.325
</code></pre></div></div>

<p>هر چقدر مقدار داخل لگاریتم بزرگتر باشه، لگاریتم اون مقدارم بزرگتر می شه. مثلا فرض کنید تعداد کل جملات 40 باشه مقدار لگاریتم می شه 1.6 یعنی بیشتر از مقدار قبل. پس طبق این فرمول هر چقدر یک کلمه در متن کمتر تکرار شده باشه، عددی که تولید می شه بزرگتره. <br />
و اگر یک کلمه در یک جمله خیلی تکرار شده باشه ولی در کل متن خیلی کم باشه، مقدار نهایی عدد بزرگی می شه.<br />
به طور خلاصه این روش کمک می کنه کلمات مهم ولی نادر رو در متن پیدا کنید.</p>

<p>مثل روش های قبل دیتا رو می خونیم و یک تابع برای پاکسازی دیتا می نویسیم. این تابع رو مثل روش اول می نویسیم:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def clean_text(text):
  text = "".join([word.lower() for word in text if word not in string.punctuation])
  tokens = re.split('\W+', text)
  text = [ps.stem(word) for word in tokens if word not in stopwords]
  return text
</code></pre></div></div>

<p>و سپس وکتورایز tf-idf رو می سازیم:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from sklearn.feature_extraction.text import TfidfVectorizer

tfidf_vect = TfidfVectorizer(analyzer=clean_text)
X_tfidf = tfidf_vect.fit_transform(dataset['body'])
</code></pre></div></div>

<p>برای اینکه یه دیدی بگیریم بهتره یه بخشش کوچکی از دیتا رو انتخاب کنیم و دیتافریم ماتریس رو بسازیم تا خروجی رو ببینیم. برای ایجاد ماتریس از تابع <code class="language-plaintext highlighter-rouge">toarray()</code> و دیتافریم پانداس استفاده می کنیم:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>X_tfidf_df = pd.DataFrame(X_tfidf_sample.toarray())
</code></pre></div></div>

<p>کد کامل <a href="Uhttps://github.com/spacelover1/NLP-with-Python/blob/main/3-VectorizingRawData/TF_IDF.ipynbRL">اینجا</a> قرار داره.</p>

</div>]]>
    </content>
  </entry>

  <entry>
    <title>مبانی پردازش زبان طبیعی(NLP)- دو</title>
    <link rel="alternate" type="text/html" href="https://spacelover.ir/nlp-basics-two.html"/>
    <id>urn:uuid:</id>
    <updated>2021-07-14T00:00:00Z</updated>
    <category term="nlp"/><category term="اموزش"/>
    <content type="html">
      <![CDATA[<div dir="rtl"><p>در قسمت قبل فایل متنی رو به دو روش آسان و دشوار خوندیم. و پاکسازی دیتا رو توسط سه متد پیش پردازش یاد گرفتیم: حذف علائم نگارشی، توکنایز کردن (جداسازی کلمات)، حذف کلمات بدون معنی. و گفتیم که یک مرحله چهارمی هم برای پاکسازی یا نرمالسازی داده متنی می شه استفاده کرد که شاید همیشه به اندازه مراحل قبل اهمیت نداشته باشه. در این قسمت درباره این مرحله چهارم صحبت می کنیم.</p>

<h2 id="بخش-پنجم-stemming"><strong>بخش پنجم: Stemming</strong></h2>

<p>کاری که stemming انجام می ده اینه که میاد پسوند و پیشوند رو از کلمه حذف می کنه و هر چی باقی موند رو به عنوان خروجی میده، پس ممکنه خروجی حتی کلمه نباشه. خب پس چرا ازش استفاده می کنیم؟ بخاطر سادگی و سرعتی که داره.</p>

<p>اول از همه روی یک سری کلمات این روش رو اجرا می کنیم تا بیشتر باهاش آشنا شیم و بعد روی دیتاستی که در قسمت قبل بررسی کردیم. <br />
باید اول کتابخونه <code class="language-plaintext highlighter-rouge">nltk</code> رو ایمپورت کنیم. سپس از  استمر <code class="language-plaintext highlighter-rouge">PorterStemmer()</code>  استفاده می کنیم. این استمرها برای هر زبانی متفاوته، برای زبان انگلیسی دو استمر <code class="language-plaintext highlighter-rouge">PorterStammer</code> و <code class="language-plaintext highlighter-rouge">LancasterStammer</code> وجود داره. اولی رایجتره و سریعتر.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import nltk
ps = nltk.PorterStemmer()
</code></pre></div></div>

<p>با استفاده از <code class="language-plaintext highlighter-rouge">dir(ps)</code> توابعی که این استمر داره رو می تونیم مشاهده کنیم. تابع <code class="language-plaintext highlighter-rouge">stem</code> بیشتر استفاده می شه.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>print(ps.stem('grow'))
print(ps.stem('growing'))
print(ps.stem('grows'))
</code></pre></div></div>

<p>همه این کلمات رو خلاصه می کنه به grow. در مثال بعدی فرق فعل و فاعل رو می دونه.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>print(ps.stem('runs'))  
print(ps.stem('running'))
print(ps.stem('runner'))
</code></pre></div></div>

<p>حالا بریم سراغ دیتاست پیام های اسپم و غیر اسپم.<br />
ابتدا دیتا رو می خونیم:
    import pandas as pd
    import re
    import string</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>nltk.download('stopwords')
stopword = nltk.corpus.stopwords.words('english')
pd.set_option('display.max_colwidth', 100)

dataset = pd.read_csv('SMSSpamCollection.tsv', sep='\t', header=None)
dataset.columns = ('label', 'body')
dataset.head()
</code></pre></div></div>

<p>کتابخونه های مورد نیاز رو هم من همین ابتدا ایمپورت کردم. سپس سه مرحله پاکسازی دیتا رو انجام  می دیم:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def clean_data(text):
    text = [word for word in text if word not in string.punctuation]
    tokens = re.split('\W+', text)
    text = [word for word in tokens if word not in stopword]
    return text

dataset['tokenized_text'] = dataset['body'].apply(lambda x: clean_data(x))



def stemming(tokenized_text):
    text = [ps.stem(word) for word in tokenized_text]
    return text

dataset['stemmed_text'] = dataset['tokenized_text'].apply(lambda x: stemming(x))
</code></pre></div></div>

<h2 id="بخش-ششم-lemmatization"><strong>بخش ششم: Lemmatization</strong></h2>

<p>همونطور که دیدیم خروجی <code class="language-plaintext highlighter-rouge">stemming</code> لزوما کلمه نیست و ممکنه یک چیز بی معنی باشه یا حتی اشتباه. یک متد دیگه <code class="language-plaintext highlighter-rouge">Lemmatization</code> نام داره که خروجی این روش حتما کلمه ای در دیکشنریه. یعنی معمولا کلمات رو می بره به ریشه شون.</p>

<p>به مثال های زیر توجه کنید.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>print(ps.stem('meaning')) ==&gt; mean
print(ps.stem('meanness')) ==&gt; mean

print(wn.lemmatize('meaning')) ==&gt; meaning
print(wn.lemmatize('meanness')) ==&gt; meanness


print(ps.stem('goose')) ==&gt; goos
print(ps.stem('geese')) ==&gt; gees

print(wn.lemmatize('goose')) ==&gt; goose
print(wn.lemmatize('geese')) ==&gt; goose
</code></pre></div></div>

<p>در واقع stemming رویکرد الگوریتمی داره و فقط با رشته ای که بهش می دیم کار می کنه و فقط پسوند رو حذف می کنه.<br />
اما lemmatization پیچیده تره و کلمه ای که بهش داده می شه رو در لیست لغات بررسی می کنه و پرداش می کنه و بعد ریشه کلمه رو بر می گردونه مشکلش اینه که اگر کلمه ای که بهش داده شده در لیست لغات نباشه همونو برمی گردونه.</p>

<p>همین اتفاقی که در این مثال ها افتاده، که همونطور که می بینیم خلاصه نکردن بهتر از رشته کلمه اشتباه برگردوندنه.</p>

<p>حالا می خوایم تکنیک lemmatization رو روی دیتاست پیام ها پیاده کنیم. مثل قبل، ابتدا دیتا رو می خونیم و پاکسازی های اولیه رو انجام می دیم و بعد از lemmatizer استفاده می کنیم. <br />
در اینجا من فقط تابع lemmatizer رو نوشتم. کد کامل رو <a href="https://github.com/spacelover1/NLP-with-Python/blob/main/2-SupplementalDataCleaning/UsingaLemmatizer.ipynb">اینجا</a> می تونید مشاهده کنید.</p>

<p>برای درک بهتر این دو روش می تونید <a href="https://www.datacamp.com/community/tutorials/stemming-lemmatization-python">این توضیح انگلیسی</a> رو مطالعه کنید.</p>

</div>]]>
    </content>
  </entry>

  <entry>
    <title>مبانی پردازش زبان طبیعی(NLP)- یک</title>
    <link rel="alternate" type="text/html" href="https://spacelover.ir/nlp-basics-one.html"/>
    <id>urn:uuid:</id>
    <updated>2021-07-10T00:00:00Z</updated>
    <category term="nlp"/><category term="اموزش"/>
    <content type="html">
      <![CDATA[<div dir="rtl"><p>این پست رو با یک نقل قول شروع می کنم:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>"Motivation often comes after starting, not before. Action produces momentum."
</code></pre></div></div>

<p>تعاریف متفاوتی برای پردازش زبان طبیعی وجود داره ولی شاید کامل ترینش این باشه:</p>

<blockquote>
  <p>پردازش زبان طبیعی شاخه ای از دانشه که بر روی توانایی کامپیوتر برای درک، تحلیل، تغییر و احتمالا تولید زبان انسان متمرکز است.</p>
</blockquote>

<p>در چند سری پست آینده می خوام یک سری مبانی برنامه نویسی مربوط به پردازش زبان طبیعی رو توضیح بدم، کدها هم در <a href="https://github.com/spacelover1/NLP-with-Python"><u>این ریپو</u></a> قرار دارند. در این آموزش فرض شده که شما با مبانی پایتون یا حداقل برنامه نویسی آشنا هستید چون اینجا مبانی، مثل استفاده از لیست و دیگر نوع داده ها آموزش داده نمی شه.</p>

<p>زبانی که در این سری استفاده می شه پایتونه، پس نیازه که پایتون رو نصب کنید. برای محیط برنامه نویسی هم می تونید از <a href="https://www.jetbrains.com/pycharm/download/#section=windows"><u>پایچارم</u></a> یا ژوپیتر استفاده کنید. برای استفاده از ژوپیتر باید <a href="https://docs.anaconda.com/anaconda/"><u>آناکوندا</u></a> رو نصب کنید. لینک سایت اصلی این برنامه ها رو هم قرار دادم که برای دانلود می تونید استفاده کنید. حتما دقت کنید که هر برنامه رو برای سیستم عامل خودتون دانلود کنید (خود این سایت ها به صورت خودکار سیستم عامل رو شناسایی می کنند).</p>

<h2 id="بخش-اول-خواندن-فایل"><strong>بخش اول: خواندن فایل</strong></h2>

<p>خب شروع کنیم. تو این قسمت قراره یک فایل متنی رو بخونیم و یک سری مرتب سازی ها روش انجام بدیم. اول یک با دیتاست بازی می کنیم تا  یکسری اطلاعات از دیتاست دستمون بیاد و بعد یک روش ساده برای خوندن این دیتاست معرفی می کنیم. <br /> 
فایلی که استفاده می شه در ریپویی که لینکش رو بالا گذاشتم موجوده. برای خوندن فایل باید اول با استفاده از تابع <code class="language-plaintext highlighter-rouge">open()</code> فایل رو باز کنیم و با تابع <code class="language-plaintext highlighter-rouge">read()</code> فایل باز شده رو می خونیم و محتواش رو در یک متغیر به اسم <code class="language-plaintext highlighter-rouge">raw_data</code> می نویسیم.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>raw_data = open('file_name').read()
</code></pre></div></div>

<p>خب حالا باید ببینیم محتوای این <code class="language-plaintext highlighter-rouge">raw_data</code> به چه صورته تا ببینیم نیاز به مرتب سازی داره برای پردازش های بعدی یا نه. فایل رو اگر دیده باشید داده نه ساختاریافته نیست و خیلی هم بدون ساختار نیست، اول هر خط کلمه ham یا spam داره و بعد با یه تب فاصله یک متنی جلوش نوشته شده. بریم چند خط از فایل رو ببینیم:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>raw_data[0:500]
</code></pre></div></div>

<p>بعد از اجرای این خط 500 کاراکتر اول فایل نمایش داده می شه. همون طور که می بینید محتوای فایل یک سری رشته است که کاراکترهایی مثل <code class="language-plaintext highlighter-rouge">\t</code> و <code class="language-plaintext highlighter-rouge">\n</code> داره. اولی فاصلی ای مه باتب ایجاد شده رو نشون می ده و دومی باعث می شه بره خط بعدی.
خب ما الان می خوایم یکم داده رو مرتب کنیم، برای پردازش های بعدی. برای اینکار کاراکتر <code class="language-plaintext highlighter-rouge">\t</code> رو با <code class="language-plaintext highlighter-rouge">\n</code> جایگزین می کنیم و بعد با تابع <code class="language-plaintext highlighter-rouge">split</code> این رشته رو به لیست تبدیل می کنیم.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>parsed_data = raw_data.replace('\t','\n').split('\n')
</code></pre></div></div>

<p>بریم ببینیم لیست مون چه شکلیه:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>parsed_data[:5]
</code></pre></div></div>

<p>الان برچسب یا لیبل در خونه های با ایندکس زوج قرار داره و متن پیام در خونه های ایندکس فرد. حالا که یه نظمی تو ساختار ایجاد شده بریم هر کدوم رو تو لیست جدا بنویسیم:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>label_list  = parsed_data[0::2]
text_list = parsed_data[1::2]
</code></pre></div></div>

<p>اولی لیست رو از خونه صفر می خونه و یکی درمیون مقادیر رو می ریزه تو لیست لیبلا، یعنی همون خونه های زوج. خط دوم از خونه یک میاد یکی در میون می خونه و باعث می شه خونه های فرد رو بخونه.<br />
می تونیم این دو لیست رو ببینیم و مطمئن شیم همونطور که می خواستیم شده. اینجا از تابع <code class="language-plaintext highlighter-rouge">print()</code> استفاده می کنیم چون در غیر اینصورت ژوپیتر فقط خروجی اخرین خط رو می ده.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>print(label_list[:5])
print(text_list[:5])
</code></pre></div></div>

<p>حالا باید این دو لیست رو یه جوری با هم ترکیب کنیم تا برای تحلیل های بعدی بتونیم ازشون استفاده کنیم. برای این کار از تابع <code class="language-plaintext highlighter-rouge">DataFrame()</code> پانداس استفاده می کنیم و در این دیتافریم یک دیکشنری می سازیم. بعد از ایمپورت کتابخونه می تونیم از توابعش استفاده کنیم:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import pandas as pd
full_corpus = DataFrame({'label': label_list, 'body': text_list})
</code></pre></div></div>

<p>احتمالا بعد از اجرای این خط کد اروری مشاهده می کنید که می گه ارایه ها باید طولشون یکسان باشه. بریم طول این دو لیست رو چک کنیم:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>print(len(label_list))
print(len(text_list))
</code></pre></div></div>

<p>همون طور که می بینید طول لیست لیبل یه دونه بیشتر از متنه. بریم چند تا خونه اخر هر دو لیست رو ببینیم چه خبره:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>label_list [-5:]
text_list[-5:]
</code></pre></div></div>

<p>خونه اخر لیست لیبلا یه خونه خالیه. پس وقتی این لیست رو می خونیم، خونه اخر رو باید نادیده بگیریم:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>full_corpus = DataFrame({'label': label_list[:-1], 'body': text_list})
</code></pre></div></div>

<p>ببینیم دیتافریم مون چه شکلیه:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>full_corpus.head()
</code></pre></div></div>

<p>خب الان دیتامون یکم ساختار پیدا کرده و نسبت به اول خوانایی بهتری داره. <br /></p>

<p>و اما روش ساده خوندن این دیتاست. همون اول که نگاهی به داده انداختیم و <code class="language-plaintext highlighter-rouge">\t</code> رو دیدیم باید متوجه شیم که این یک فایلیه که عناصرش با تب از هم جدا شدن. برای خوندن این فایل ها می تونیم از تابع <code class="language-plaintext highlighter-rouge">read_csv</code> استفاده کنیم، مقدار هدر رو هم در اینجا <code class="language-plaintext highlighter-rouge">None</code> قرار می دیم چون در فایل اصلی ردیفی برای عنوان ستون وجود نداره.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>dataset = read_csv('file_name', sep="\t", header=None)
</code></pre></div></div>

<p>برای مشاهده هم می تونیم از تابع <code class="language-plaintext highlighter-rouge">head()</code> استفاده کنیم:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>dataset.head()
</code></pre></div></div>

<p>همونطور که می بینید دیگه ردیف اولی برای عنوان ستون ها وجود نداره.</p>

<p>در این بخش یاد گرفتیم که یک دیتاست متنی رو چطوری بخونیم و برای تحلیل های بعدی مرتبش کنیم.
کدهای بخش اول رو از <a href="https://github.com/spacelover1/NLP-with-Python/blob/main/1-Basics/01-read_file.ipynb"><u>اینجا</u></a> می تونید ببینید.</p>

<h2 id="بخش-دوم-بررسی-دیتاست"><strong>بخش دوم: بررسی دیتاست</strong></h2>

<p>بخش قبل یاد گرفتیم که چطور دیتا رو به روش پیچیده بخونیم تا با یک سری از ابزارهای تغییر در متن آشنا شیم و در آخر روش ساده رو یاد گرفتیم:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>dataset = read_csv('file_name', sep="\t", header=None)
</code></pre></div></div>

<p>حالا برای ستون ها اسم برچسب می ذاریم تا راحت بتونیم دیتا رو بررسی کنیم:
    dataset.columns[‘label’, ‘body’]</p>

<p>اول از همه بریم سایز دیتاست رو دربیاریم، ببینیم چند تا سطر و ستون داره:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>print("There are {} rows and {} columns.".format(len(dataset), len(dataset.columns)))
</code></pre></div></div>

<p>حالا باید تعداد سطرهای اسپم و غیر اسپم رو دربیاریم:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>print("Out of {} rows, {} rows are spam and {} rows are ham.".format(
                                                                    len(dataset),
                                                                    len(dataset[dataset['label'] == 'spam']),
                                                                    len(dataset[dataset['label'] == 'hame'])
                                                                    ))
</code></pre></div></div>

<p>ممکنه بعضی ردیفا هیچ لیبلی نداشته باشه، باید تعداد اون ها رو هم پیدا کنیم:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>print("Number of null in label: {}".format(dataset['label'].isnull().sum()))
print("Number of null in body: {}".format(dataset['body'].isnull().sum()))
</code></pre></div></div>

<p>خروجی تابع <code class="language-plaintext highlighter-rouge">isnull()</code> بولینه یعنی True یا False برمی گردونه. هر سطر رو بررسی می کنه اگر اون ستونی که داریم بررسی می کنیم تو اون سطر دیتا نداشته باشه True میشه و در غیراینصورت False. 
وقتی بعد از <code class="language-plaintext highlighter-rouge">isnull()</code> تابع <code class="language-plaintext highlighter-rouge">sum()</code> رو میاریم یعنی تعداد کل ریف هایی که لیبل ندارن رو بده.</p>

<p>کدهای بخش دوم رو از <a href="https://github.com/spacelover1/NLP-with-Python/blob/main/1-Basics/02-Exploring%20Dataset.ipynb"><u>اینجا</u></a> می تونید ببینید.</p>

<h2 id="بخش-سوم-عبارات-منظم"><strong>بخش سوم: عبارات منظم</strong></h2>

<p>دلیل اصلی یاد گرفتن عبارات با قاعده اینه که بتونیم جمله رو tokenize کنیم یا به عبارتی کلمه های جمله رو از هم جدا کنیم که پایتون بدونه که دنبال چی بگرده. گاهی اوقات لازمه که یک الگوی خاصی از کاراکترها رو در یک رشته متن پیدا کنیم، این کار به راحتی با عبارات با قاعده قابل انجامه. البته خیلی راحتم نه :) چون روش هایی که برای تشخیص این الگوها وجود داره خیلی گسترده است ولی یک سری موارد کلی رو اگر یاد بگیرید و تمرین کنید براتون راحت می شه. از <a href="https://docs.python.org/3/library/re.html">داکیومنت عبارات با قاعده در پایتون</a> هم می تونید استفاده کنید.</p>

<p>در اینجا دو روش رو یاد می گیریم. برای تست این دو روش یک متن رو به سه حالت نوشتم و قراره که کلمه ها رو از هم جدا کنیم.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import re
text = "This is a made up string to test 2 different regex methods"
messy_text = "This            is a made up string to              test 2   different regex methods"
messy_text1 = "This-is-a''''made-up/string-+to*****test-2 &gt;&gt;&gt;&gt;&gt;&gt;-different.regex-methods"
</code></pre></div></div>

<p>برای اینکه بتونیم از این عبارات با قاعده در پایتون استفاده کنیم باید کتابخونه ش رو ایمپورت کنیم. <br />
کلمات جمله اول با فاصله از هم جدا شدند پس یه الگو به دست اومد، می تونیم هر جا کاراکتر فاصله بود تشخیص بدیم و کلمات رو از هم جدا کنیم. با استفاده از تابع <code class="language-plaintext highlighter-rouge">split()</code> و ‘\s’ به کوچک و بزرگ بودن حروف دقت کنید، چون هر کدوم معنی خاصی دارند. در اینجا حرف کوچک s یعنی از نقاطی که کاراکتر فاصله وجود داره کلمات رو جدا کن.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>re.split('\s', 'text')
</code></pre></div></div>

<p>اگر همین کد رو روی متن دوم اجرا کنیم جوابی که می خوایم رو نمی گیریم، چون بیشتر از یک کاراکتر فاصله بین بعضی کلمات وجود داره. پس از ‘\s+’ استفاده می کنیم. این یعنی یک فاصله یا بیشتر اگر بین کلمات بود حذف کن.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>re.split('\s+', 'messy_text')
</code></pre></div></div>

<p>برای متن سوم هیچ کدوم از راه حلای بالا جواب نمی ده. و باید از راه دیگه ای بریم. اینجا به غیر از کاراکتر فاصله، کاراکترهای دیگه ای هم وجود داره. پس می تونیم بیاییم بگیم هر کاراکتر غیر کلمه ای رو حذف کن. و علامت + رو هم می ذاریم چون کاراکترهای غیرکلمه بیشتر از یکبار تکرار شدن بین هر کلمه.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>re.split('\W+', 'messy_text1')
</code></pre></div></div>

<p>کاری که تا الان انجام می دادیم این بود که بیاییم هر چی غیر از کلمه است رو جدا می کردیم تا کلمه ها جدا شن. یه راه دیگه برای جداسازی کلمات اینه که بیاییم مستقیم کلمه ها رو پیدا کنیم. برای این کار از تابع <code class="language-plaintext highlighter-rouge">findall()</code> استفاده می کنیم.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>re.findall('\S+', text)
</code></pre></div></div>

<p>در کد بالا از حرف بزرگ S استفاده کردیم. گفتیم هر چی غیر کاراکتر فاصله. که این روش دوباره روی متن سوم جواب نمیده. که می تونیم از روش زیر به جای این استفاده کنیم.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>re.findall('\w+', 'messy_text1')
</code></pre></div></div>

<p>حرف کوچک w رو اینجا استفاده کردیم. یعنی کاراکترهای کلمه رو جدا کن.</p>

<p>کدهای بخش سوم <a href="https://github.com/spacelover1/NLP-with-Python/blob/main/1-Basics/03-RegEx.ipynb"><u>اینجا</u></a> قرار دارند.<br />
یک کار دیگه ای که می شه با این عبارات با قاعده انجام داد اینه که کلماتی که املاشون اشتباه نوشته شده در یک متن رو پیدا کرد و با مقدار درستش جایگزین کرد. مثال این مورد رو می تونید در <a href="https://github.com/spacelover1/NLP-with-Python/blob/main/1-Basics/04-RegEx1.ipynb">اینجا</a> مشاهده کنید.</p>

<h2 id="بخش-چهارم-پیش-پردازش-پاکسازی-داده"><strong>بخش چهارم: پیش پردازش (پاکسازی) داده</strong></h2>

<p>برای اینکه داده آماده بررسی و تحلیل بشه باید یکسری مراحل به عنوان پیش پردازش روش انجام بشه تا مواردی که اضافه است حذف بشه. این مراحل شامل حذف علائم نگارشی، تقسیم جمله به کلمه ها متشکل، حذف کلماتی که معنی خاصی ندارندو حذف مشتقات کلمات می شود.</p>

<p>بریم پنج سطر اول دیتا رو دوباره ببینیم:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import pandas as pd
pd.set_option('display.max_colwidth', 100)
dataset = pd.read_csv('file_name', sep='\t', header=None)
dataset.columns = ['label', 'body']
dataset.head()
</code></pre></div></div>

<p>خط دوم <code class="language-plaintext highlighter-rouge">set_option('display.max_colwidth', 100)</code> تعداد کاراکترهایی که نمایش داده می شه رو مشخص می کنه. <br />
الان دیتا به این صورته:</p>

<p><img src="https://raw.githubusercontent.com/spacelover1/personalBlog/master/image/dataset.PNG" alt="dataset_before_cleaning" /></p>

<p>فایل دیتاست بعد از پاکسازی هم در این فولدر وجود داره و قراره دیتا به این صورت دربیاد:</p>

<p><img src="https://raw.githubusercontent.com/spacelover1/personalBlog/master/image/cleaned_dataset.PNG" alt="cleaned_dataset" /></p>

<h3 id="حذف-علائم-نگارشی"><strong>حذف علائم نگارشی</strong></h3>

<p>علائم نگارشی در کتابخونه <code class="language-plaintext highlighter-rouge">string</code> قرار دارند. باید یک تابع بنویسیم که این علائم نگارشی رو از متن پیام در دیتا حذف کنه. و متن بدون علائم نگارشی بده.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def remove_punct(text):
    text_nopunct = [char for char in text if char not in string.punctuation]
    return text_nopunct

dataset['body_nopunct'] = dataset['body'].apply(lambda x: remove_punct(x))
</code></pre></div></div>

<p>در اینجا چون متن پیام رو داره کاراکتر  به کاراکتر بررسی می کنه در نهایت هم کاراکتر ها رو از هم جدا می کنه و به عنوان خروجی می ده، برای اینکه به صورت کلمه خروجی بگیریم از تابع <code class="language-plaintext highlighter-rouge">join()</code> استفاده می کنیم.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>text_nopunct = "".join([char for char in text if char not in string.punctuation])
</code></pre></div></div>

<h3 id="جداسازی-کلمات"><strong>جداسازی کلمات</strong></h3>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import re
def tokenize(text):
    tokens = re.split('\W+', text)
    return tokens
    
dataset['body_tokenized'] = dataset['body_nopunct'].apply(lambda x: tokenize(x.lower()))
</code></pre></div></div>

<p>تابع <code class="language-plaintext highlighter-rouge">lower()</code> شاید اینجا زیاد استفاده نشه و اهمیتش مشخص نباشه ولی چون در پایتون حروف کوچک و بزرگ یکسان نیستند باید همه حروف در کلمات یک جور باشند.</p>

<h3 id="حذف-کلمات-بدون-معنی"><strong>حذف کلمات بدون معنی</strong></h3>

<p>هر زبانی یک سری کلمات داره که معنی خاصی در جمله ندارند و اگر حذف شوند جمله معنی خودش رو حفظ می کنه. مثل حروف ربط. با استفاده از کتابخانه <code class="language-plaintext highlighter-rouge">nltk</code> این کلمات رو از جمله حذف می کنیم:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import nltk
stopwords = nltk.corpus.stopwords.words('english')
</code></pre></div></div>

<p>حالا یک تابع می نویسیم که این کلمات رو حذف کنه از جملات:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def remove_stopwords(tokenized_list):
    text_nostop = [word for word in tokenized_list if word not in stopwords]
    return text_nostop

dataset['body_nostop'] = dataset['body_tokenized'].apply(lambda x: remove_stopwords(x))
</code></pre></div></div>

<p>کدهای بخش چهارم <a href="https://github.com/spacelover1/NLP-with-Python/blob/main/1-Basics/05-cleaning%20text_1.ipynb"><u>اینجا</u></a> قرار دارند.</p>

</div>]]>
    </content>
  </entry>

  <entry>
    <title>چرا اینجا هستیم؟</title>
    <link rel="alternate" type="text/html" href="https://spacelover.ir/why-are-we-here.html"/>
    <id>urn:uuid:</id>
    <updated>2021-07-09T00:00:00Z</updated>
    <category term="هدف"/><category term="زندگی"/>
    <content type="html">
      <![CDATA[<div dir="rtl"><blockquote>
  <p>چرا اینجام؟</p>
</blockquote>

<p>یادمه این سوال رو وقتی 19 سالم بود پرسیدم و اون زمان ذهنم خیلی درگیرش بود دلیلش شاید دیدن زندگی پدربزرگ(حاج آقا) و مادربزرگ(خان ننه) مادرم بود. تا پنج سال پیش تقریبا ماهی یک یا دوبار می دیدمشون و وقتی اونا رو می دیدم یک جور غم وجودم رو می گرفت. اون زمان اون ها در دهه 80 و 90 زندگی شون بودن. صبح که بیدار می شدن، بعد از صبحانه حاج آقا می رفت باغ و اونجا می نشست، دیگه خیلی توان رسیدگی به باغ رو نداشت برای همین معمولا می رفت باغ تا زیر سایه درخت دراز بکشه یا تو باغ راه بره. تو این مدت خان ننه ناها رو آماده می کرد، ظهر حاج اقا میومد خونه و نمازش رو می خوند و بعد با هم ناهار می خوردن و بعد از ناهارم چرت می زدن. خان ننه تو خونه بود و غذا درست می کرد و بعضی وقتا درز لباسی می دوخت و از این دست کارا. بعضی روزا هم حاج اقا می رفت جلوی مسجد روستا می نشست، مردای روستا معمولا اونجا جمع می شن.
همه این کارا تا پنج سال پیش انجام می شد. یادمه یه بار حاج اقا می گفت کاش وقتی ادما پیر می شن اونا رو می نداختن جلوی گرگا تا بخورنشون. این یعنی خیلی داره سختی می کشه و من از دیدن اون وضعیت خیلی ناراحت می شدم. یک چیز دیگه ای که خیلی ناراحت کننده بود این بود که اصلا با هم حرف نمی زدن. این برای من از همه چیز غم انگیزتر بود. تا اینکه دو سه سال پیش خان ننه از این دنیا رفت نمی دونم کجا! از اون موقع حاج اقا فقط می خوابه تو خونه.</p>

<p>اوایل بیست سالگی این شعر رو خیلی دوست داشتم: <br />
روزها فکر من این است و همه شب سخنم/         که چرا غافل از احوال دل خویشتنم<br />
از کجا آمده ام، آمدنم بهر چه بود/      به کجا می روم آخر ننمایی وطنم</p>

<p>به مرور زمان فهمیدم که همه این درگیری ذهنی رو دارند و کسی جوابی براش نداره ولی این سوال همیشه تو ذهن آدم هست و بعضی وقتا (شاید بیشتر وقتایی که ناراحتیم) خیلی بیشتر بهش فکر می کنیم.</p>

<p>هر کسی سعی می کنه یه معنی ای به زندگیش بده و دنیا رو از اون منظر می بینه. <br />
بعضیا عشق به یک فرد دیگه رو معنی و دلیل زندگی شون می دونن. مثل <a href="https://www.youtube.com/watch?v=7YDkrJaiCrw"><u>این آهنگ از Aurora</u></a>.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>And every single time I run into your arms
I feel like I exist for love
</code></pre></div></div>

<p>زندگی کلا عجیبه و انسان بودن خیلی سخته. بعضی وقتا به پایین ترین حد انرژی و اعتماد به نفس می رسم. اون زمان، تمامی کارایی که انجام دادم یادم می ره. کارایی که قبل از انجامش برام شاید ناممکن بود. ولی اون زمان فقط فکر می گه هیچ کاری نمی تونم انجام بدم. برعکس یه زمانایی هست که انقدر اعتماد به نفس بالای دارم که حس می کنم هیچ چیز جلودارم نیست، هیچ مانعی سر راهم نمی بینم و هر کاری به نظرم ممکن میاد. اون زمان به این فکر می کنم که انسان چقدر می تونه قدرتمند باشه و اینکه هر فرد کارهایی می تونه انجام بده که خودش هم از توانایی ها و قابلیت هاش خبر نداره.</p>

<p>درباره این موضوع می شه کتاب ها نوشت و حرف برای گفتن زیاده. بعضی اتفاقا ممکنه باعث شه این سوال دوباره تو ذهن آدم پررنگ شه. مثلا آشنایی با یک سری افراد. چند روز پیش یه پادکستی درباره <a href="https://en.wikipedia.org/wiki/William_James_Sidis"><u>William James Sidis</u></a> گوش کردم. گفته می شه که احتمالا باهوش ترین انسانی بوده که تا حالا زندگی کرده ولی به خاطر مسائل و مشکلاتی که براش به وجود میاد اثر قابل توجهی از خودش به جا نگذاشته.</p>

<p>در انتها پیشنهاد می کنم <a href="https://www.youtube.com/watch?v=5y8HVM5g-WU">آهنگ Lucky از Aurora</a> رو گوش بدید.</p>

<p>احتمالا در آینده هم پست هایی در این زمینه بنویسم پس این داستان ادامه دارد …</p>

<p>جرقه نوشتن این پست یه جورایی پست های <a href="https://us12.campaign-archive.com/home/?u=82e3799bafd7e45119c16cfd6&amp;id=28642a5ff6"><u>خبرنامه جاش ردنر(Josh Radnor)</u></a> بوده، این خبرنامه برام جذابه و دنبالش می کنم، شما هم می تونید عضو خبرنامه شید.</p>

<p><a href="https://why-are-we-here.simplecast.com/"><u>این پادکست (Why are we here?)</u></a> هم که هم اسم عنوان این پسته در یکی از پست های این خبرنامه معرفی شده است.</p>

</div>]]>
    </content>
  </entry>

  <entry>
    <title>داری زندگی می‌کنی یا فقط نفس می‌کشی؟</title>
    <link rel="alternate" type="text/html" href="https://spacelover.ir/are-you-living-or-just-breathing.html"/>
    <id>urn:uuid:</id>
    <updated>2021-06-20T00:00:00Z</updated>
    <category term="زندگی"/><category term="مردگی"/>
    <content type="html">
      <![CDATA[<div dir="rtl"><p>؟</p>
</div>]]>
    </content>
  </entry>

  <entry>
    <title>ورزش برای لاغر شدن نیست</title>
    <link rel="alternate" type="text/html" href="https://spacelover.ir/always-exercise.html"/>
    <id>urn:uuid:</id>
    <updated>2021-06-11T00:00:00Z</updated>
    <category term="ورزش"/><category term="سلامتی"/>
    <content type="html">
      <![CDATA[<div dir="rtl"><p>“تو که اندامت خوبه، چرا ورزش می کنی؟”</p>

<p>سوالی که از زمان دبیرستان دارم می شنوم و هنوزم ادامه داره که نشون میده در طی این 10 سال دانش مردم هیچ تغییری نکرده! 
هر زمان رفتم باشگاه یا در حال ورزش کردن بودم خانما این سوال رو می پرسن و من هم جوابم همیشه این بوده که مگه فقط برای لاغر شدن ورزش کرد؟! 
الان تقریبا یک سالی هست که در حیاط مجتمع طناب می زنم و از این طریق با بچه های مجتمع آشنا شدم. و اون ها هم تشویق شدن که طناب بزنن و مدتی طنابشون رو می اوردن و با هم طناب می زدیم. و مسئله ای که هست اینه که اونا هم همین سوال رو می پرسیدن که چرا ورزش می کنی تو که لاغری؟ من هم بهشون میگم ورزش برای محکم کردن ماهیچه ها و استخوان ها لازمه. اگر از مغزت کار نکشی ضعیف می شه. ماهیچه ها هم همین طورن. مزیت دیگرش هم سلامتی روحیه است. وقتی تحرک داری شاداب تری و فکرت بهتر کار می کنه و فکرای منفی ازت دور می شه.</p>

<p>یک نکته مهمی که شاید خیلی از مردم ازش اطلاع ندارن اینه که برای کاهش وزن اول از همه باید رژیم غذایی درستی داشته باشی و حالا در کنارش ورزش. در <a href="https://spacelover.ir/physical-health.html">این پست</a> کمی درباره سلامت فیزیکی نوشتم.</p>

<p>در رابطه با سلامتی و تناسب اندام می تونید پادکست انگلیسی Optimal Health Daily رو دنبال کنید، میزبان این پادکست دکتر تغذیه است و در پادکستش مطالب مفید و جالبی رو از بلاگ های در این زمینه می خونه مطرح می کنه و در انتها نظر خودش رو میگه.</p>
</div>]]>
    </content>
  </entry>

  <entry>
    <title>زنانی که با پای پیاده زمین را می پیمایند</title>
    <link rel="alternate" type="text/html" href="https://spacelover.ir/walks-the-earth.html"/>
    <id>urn:uuid:</id>
    <updated>2021-06-04T00:00:00Z</updated>
    <category term="سفر_به_درون"/>
    <content type="html">
      <![CDATA[<div dir="rtl"><p>از بین داستانایی که در لیست پیشنهادات مطالعاتی کروم بود یه داستان نظرم رو جلب کرد، <a href="http://www.bbc.com/travel/story/20210527-the-woman-who-walked-around-the-world">“زنی که پیاده دور دنیا رفت”</a>. اولش گفتم از این داستانای حوصله سربر کلیشه ایه و بیخیالش شدم. روز بعد دوباره تو لیست دیدمش. یه نگاهی انداختم و غرقش شدم. کاری که من دوست دارم انجام بدم رو انجام داده بود. خب یه خلاصه ای از داستانش رو بگم و بعد هم از فکرای خودم.</p>

<p>انجلا مکسول یک خانم اهل امریکاست که در اوایل دهه سی سالگیش که می شه سال 2013 تمام وسایل خونه ش رو می فروشه و یه سفر به دور دنیا با پای پیاده رو شروع می کنه. هدفش رو ارتباط عمیق تر با دنیا تعریف کرده. اواخر سال 2020 سفرش به پایان رسید و در حال حاضر روی کتابش درباره همین سفر داره کار می کنه. انجلا به گفته خودش یه بیزنسی رو راه انداخته بود و در یک رابطه عاطفی هم بوده و اینطوری نبوده که سفرش از روی غم یا از دست دادن چیزی یا کسی بوده باشه.</p>

<p>در این گزارش بی بی سی، انجلا اسم چند خانم دیگه رو هم آورده که تقریبا تجربه مشابهی رو داشتند. و همشون هم کتاب نوشتن درباره این تجربه شون. یکیشون که فیلم یکی از کتاباش ساخته شده خانمی به اسم “رابی دیویدسون”. امروز فیلم “رد پاها(Tracks)” رو دیدم که درباره کتابی به همین اسمه که رابی نوشته. این فیلم داستان اولین سفر رابیه که در دهه 20 سالگی با چند تا شتر بیابونای استرالیا رو طی می کنه. طبق فیلم، مادر رابی خودکشی کرده بود و پدرش هم رفته بود افریقا و دیگه برنگشته بود. ظاهرا بعد از این هم باز به این سفرها رفته و درباره شون کتاب نوشته.</p>

<p>سفر رفتن اون هم بدون اینکه بدونی چی در انتظارته خیلی هیجان انگیزه البته منظورم بدون برنامه و فکر پیش رفتن نیست. سال 99 من تصمیم گرفتم برای کار داوطلبانه برم جنوب و تو راه به هر کی می گفتم از تهران اومدم برای چه کاری همه می گفتن “چرا؟” یا “تهران خیلی دوره!” یا “اینجا کجا تهران کجا!” و اینکه اصلا سفر برای انجام کار داوطلبانه رو درک نمی کردن. درباره این سفرم مفصل تو <a href="https://spacelover.ir/volunteer-work-trip.html">این پست</a> نوشتم. داشتم پایان نامه مو می نوشتم که برای اینکه خودم رو تشویق کنم که بنویسمش و تموم شه به خودم قول دادم که یه سفر به جنوب برم با اینکه هوای جنوب اون موقع شاید برای سفر مناسب نباشه ولی پایان نامه رو تموم کردم و رفتم سفر. مشکلاتی هم بود ولی خب در کل تجربه خوبی بود.</p>

<p>چند ماه بعد یعنی دی ماه یه سفر دیگه رفتم به جنوب این دفعه به قشم. این سفر هم پستی، بلندی های خودش رو داشت (که بیشتر اتفاقات ناگواری که افتاد بخاطر جنسیتم بود، یعنی مثلا کسی مزاحم پسرا نمی شه). این سفر طولانی تر بود و خیلی چیزا رو برای اولین بار در جریان این سفر تجربه کردم. یکیش سفر با اتوبوس بود که همیشه می ترسیدم ولی در این سفر از بندرعباس با اتوبوس به تهران اومدم. اینکه با مردم جدید ارتباط برقرار می کنی و با طرز فکرا و زندگی های مختلف اشنا می شی برای من خیلی جالبه و خیلی حالم رو خوب می کنه.</p>

<p>امیدوارم باز هم برم سفر!</p>
</div>]]>
    </content>
  </entry>

  <entry>
    <title>هر هفته، یک پست</title>
    <link rel="alternate" type="text/html" href="https://spacelover.ir/write-every-week.html"/>
    <id>urn:uuid:</id>
    <updated>2021-05-30T00:00:00Z</updated>
    <category term="نویسندگی"/><category term="هفته"/><category term="چالش"/>
    <content type="html">
      <![CDATA[<div dir="rtl"><p>تصمیم گرفتم که چهارشنبه هر هفته یک پست در این بلاگ یا <a href="https://spacelover1.github.io/">بلاگ انگلیسیم</a> بذارم. دیدم که بعضیا برای خودشون چالش روزانه نوشتن رو می ذارن ولی خب یا بعد یه مدت پشیمون شدن چون مطلب جدیدی برای نوشتن نداشتن یا اینکه وقتش رو نداشتن. یا یه مدت نوشتن و دیگه کلا یه مدت زیادی سراغی از بلاگ نگرفتن. برای اینکه مثل یک روهروی واقعی پیش برم فکر می کنم هفته ای یک پست خوب باشه.</p>
</div>]]>
    </content>
  </entry>

  <entry>
    <title>با تمام قدرت ادامه بده!</title>
    <link rel="alternate" type="text/html" href="https://spacelover.ir/go-with-the-most-power.html"/>
    <id>urn:uuid:</id>
    <updated>2021-04-25T00:00:00Z</updated>
    <category term="تصمیم"/><category term="شک"/><category term="پیوستگی"/>
    <content type="html">
      <![CDATA[<div dir="rtl"><div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>If the decision doesn't bring me peace and tranquility, I shouldn't make that choice. -Cecilia Suarez
</code></pre></div></div>

<p>به یه چیزی علاقه داری، همه جوانبش رو بررسی کن، چیزایی که در مسیر به دست آوردنش از دست می دی و چیزایی که وقتی بهش برسی بدست می آری. بعد از اون دیگه در مسیر به دست آوردنش حرکت کن. شک و تردید به خودت راه نده. نذار که فکرای بیهوده تو از مسیرت دورت کنند. هر چند وقت یه بار بررسی کن که تو مسیر اصلی باشی و ننداخته باشی جاده خاکی.</p>

<p>پیوستگی مهمترین اصله در رسیدن به هدف.</p>

<blockquote>
  <p>رهرو آن نيست كه گه تند و گهي خسته رود… رهرو آنست كه آهسته و پيوسته رود.(سعدي)</p>
</blockquote>

<p>لحطاتی هستند که آدم اگر حواسش نباشه میاد دوباره فکر می کنه و دوباره بررسی می کنه و از مسیر اصلی دور می شه. صورت مسُله مشخصه، راه های رسیدن به پاسخ هم مشخص کردی، پس دیگه برو دنبال انجام هر گام. مستقیم حرکت کن و برو سراغ گام بعدی تا پاسخ تکمیل شه. موقع حل مسُله هم از چالش هایی که باهاشون مواجه می شی یاد بگیر و لذت ببر چون بعد حل این سوال، باید سوالای خیلی بزرگتری رو حل کنی.</p>

<p>پس:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>You want something go get it.Period. -"The Pursuit of Happiness"

"It's the magic of risking everything for a dream that nobody sees but you." -"Million Dollar Baby
</code></pre></div></div>

</div>]]>
    </content>
  </entry>

  <entry>
    <title>چرا از حق خودمان دفاع نمی‌کنیم؟</title>
    <link rel="alternate" type="text/html" href="https://spacelover.ir/stand-up-for-your-rights.html"/>
    <id>urn:uuid:</id>
    <updated>2021-03-04T00:00:00Z</updated>
    <category term="بی‌تفاوتی"/><category term="حق"/><category term="دفاع"/><category term="دزدی"/><category term="ارزش"/>
    <content type="html">
      <![CDATA[<div dir="rtl"><p>فرض کن داری با یه کیف پر پول، تو خیابون راه می‌ری، ناغافل یکی می‌آد و کیفت رو می‌زنه، چی‌کار می‌کنی؟</p>

<p>تو دانشگاه یه استادی داشتیم که همیشه ادعای خوب و متدین بودن می‌کرد و همیشه یک ساعت اول هر جلسه می‌رفت بالای منبر و از مشکلات جامعه می‌گفت، طوری‌که انگار دانشجوها نیستن تو این جامعه و از اوضاع خبر ندارن. با مطرح کردن مشکلات مشکلی ندارم در صورتی که بحث دو طرفه باشه و قرار باشه چیزی یاد بگیریم و راه حلی مطرح شه. نه اینکه گوش مفت گیر بیاری و خودتو بخوای خالی کنی و وقتی که برای آموزش گذاشتن رو تلف کنی. و یه مورد دیگه از این استاد مدعی این که ایشون همیشه یک تا یک ساعت و نیم بعد از ساعت مقرر به کلاس تشریف‌فرما می‌شدن.&lt;/br&gt;
یعنی مثلا کلاس ساعت ۳ شروع می‌شد، ایشون ۴:۳۰ می‌اومدن. و پذیرش این واقعا برام زور داشت، تا جایی که می‌تونستم اعتراض کردم به هر جا می‌شد ولی می‌گفتن فقط شما بیایی کافی نیست. به بچه‌ها می‌گفتم که این استاد داره حق شما رو ضایع می‌کنه، دزدین همیشه کیف شما رو زدن نیست، اینم یه جور دزدیه. و جواب بچه‌ها:</p>

<p>“چقد سخت می‌گیری، تموم می‌شه دیگه!”&lt;/br&gt;
“این درس همین یه استاد رو داره اعتراض کنیم که چی بشه، تغییر که نمی‌کنه!”&lt;/br&gt;
“همین یه ترمه دیگه!”&lt;/br&gt;
“اگر بفهمه رفتیم اعتراض کردیم، می‌ندازتمون!”&lt;/br&gt;
“کسی به حرف ما گوش نمی‌کنه، اعتراض به جایی نمی‌رسه!”&lt;/br&gt;</p>

<p>واقعا از این حجم از بی‌تفاوتی حالم به‌هم می‌خورد، اما من کاری که از دستم برمی‌اومد انجام دادم.</p>

<p>اگر بچه‌ها، هر ترم می‌رفتن اعتراض می‌کردن به این وضعیت، حتما در ترم‌های آینده بررسی می‌شد و وضعیت هم‌اینطوری پیش نمی‌رفت، اما هر کی فقط به فکر خودش بود.</p>

<p>این موارد زیادن تو زندگی ما در ایران، می‌گم ایران چون من اینجا بودم فقط.</p>

<p>چرا اگر کیف پر پولت رو بزنن می‌دویی و تمام تلاشتو برای پس گرفتن کیف می‌کنی، اما زمانت رو که بارزش‌ترین سرمایه‌ توعه، کسی بدزده میگی سخت نگیر؟!</p>

<p>این دانشجوهای بی‌تفاوت، قبلا دانش‌آموزان بی‌تفاوت بودن و بعدا هم می‌شن والدین بی‌تفاوت و در نهایت شهروندان بی‌تفاوت و نتیجه می‌شه یک ملت بی‌تفاوت با سیستم فاسد.</p>

</div>]]>
    </content>
  </entry>

  <entry>
    <title>ممنونم، بیشتر لطفا :)</title>
    <link rel="alternate" type="text/html" href="https://spacelover.ir/thankyou.html"/>
    <id>urn:uuid:</id>
    <updated>2021-02-08T00:00:00Z</updated>
    <category term="شکرگزاری"/>
    <content type="html">
      <![CDATA[<div dir="rtl"><p>جاش ردنر بازیگر شخص اول سریال “How I met your mother”  یه فیلمی ساخته به اسم “happythankyoumoreplease”.</p>

<p>نکته زیاد داره به نظرم، الان درباره یکیش می‌خوام بنویسم: 
شکر نعمت، نعمتت افزون کند.</p>

<p>و حالا I wanna give gratitude a shot.</p>

<p>می‌خوام از این به بعد بگم ممنونم، برای همه چی، بیشتر لطفا؟ :)</p>

</div>]]>
    </content>
  </entry>

  <entry>
    <title>یک جمله طلایی</title>
    <link rel="alternate" type="text/html" href="https://spacelover.ir/golden-quote.html"/>
    <id>urn:uuid:</id>
    <updated>2021-01-10T00:00:00Z</updated>
    <category term="رهاکردن"/><category term="رها"/><category term="شدن"/>
    <content type="html">
      <![CDATA[<div dir="rtl"><p>بی‌حال و بی‌رمق دراز کشیده بودم روی تختم. ایمیل خبرنامه  James Clear رو دریافت کردم، خیلی وقته که دیگه جملات انگیزشی نمی‌خونم و برام مسخره‌س. این خبرنامه انگیزشی نیست و یه جور تلنگره برام جمله‌هاش و به همین خاطر دنبال می‌کنمش و می‌خونمش. جمله اول نوشته بود:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>"Life instantly improves when you don’t blame other people and focus on what you can control."
</code></pre></div></div>

<blockquote>
  <p>به محض اینکه از سرزنش دیگران دست برداری و روی موارد قابل کنترل تمرکز کنی زندگیت بهتر می‌شه.</p>
</blockquote>

<p>شاید دیگه خسته شده بودم از اون وضع و می‌خواستم حالم خوب شه، به خودم گفتم که باشه دست از سرزنش دیگران برمی‌دارم ببینم حالم خوب میشه یا نه و می‌خواستم با سفر شروع کنم، می‌خواستم یه مدت از اون محیط همیشگی دور شم.</p>

<p>و ۱۵۰۰ کیلومتر دور شدم. بدون برنامه از تهران رفتم به سوزا شهری در  قشم. 
اوایل سفر اصلا خوب نبود و به دلیل بی‌تجربگی با آدمای نامناسبی برخورد داشتم. ولی بعدش روی خوش سفر رو تجربه کردم. تو اون دو هفته‌ای که اونجا بودم حال خوبی داشتم. رها از همه چی و همه کس.</p>
</div>]]>
    </content>
  </entry>

  <entry>
    <title>توجه! نام دامنه تغییر می‌کند</title>
    <link rel="alternate" type="text/html" href="https://spacelover.ir/changing-domain-name.html"/>
    <id>urn:uuid:</id>
    <updated>2020-10-19T00:00:00Z</updated>
    <category term="بلاگ"/>
    <content type="html">
      <![CDATA[<div dir="rtl"><p>علی‌رغم میلم مجبورم اسم دومین رو تغییر بدم، تقریبا ده روز دیگه این دومین منقضی میشه و در حال حاضر نمی‌تونم هزینه پسوند نت رو پرداخت کنم.
می‌تونم از دامنه‌ای که گیت‌هاب میده استفاده کنم و یا اینکه یه پسوند دیگه تهیه کنم. راه دوم رو انتخاب کردم فعلا و از تاریخ 9 آبان (30 اکتبر) آدرس این بلاگ به spacelover.ir تغییر می‌کنه، فقط پسوندش به ir تغییر می‌کنه.</p>

</div>]]>
    </content>
  </entry>

  <entry>
    <title>سفر تنهایی- تجربه من از اولین کار داوطلبانه</title>
    <link rel="alternate" type="text/html" href="https://spacelover.ir/volunteer-work-trip.html"/>
    <id>urn:uuid:</id>
    <updated>2020-09-08T00:00:00Z</updated>
    <category term="سفر"/><category term="کارداوطلبانه"/><category term="تجربه"/>
    <content type="html">
      <![CDATA[<div dir="rtl"><p>سفر به جزایر کیش یا قشم تبدیل شد به کار داوطلبانه در بندر زیارت.</p>

<p>تقریبا نزدیک سه هفته بود که تصمیمم درباره کار داوطلبانه در یه بومگردی قطعی شده بود و خیلی هیجان داشتم که می‌خوام تنهایی برم یه شهر دور و جدید. وقتی رو نقشه نگاش می‌کردم حس می‌کردم ته دنیاست. جذابیت دیگه سفر برام، نحوه رسوندن خودم به مقصد بود. هدف اصلیم از سفر، تنها بودن با خودم جایی خیلی دور از خونه بود.</p>

<p>خلاصه بعد از کلی توضیح به خانواده، روز موعود فرا رسید. بلطیم برای چهارشنبه ۶:۲۵ صبح بود، هواپیمایی ماهان. پامو که از در خونه گذاشتم بیرون یه حس دلتنگی شدیدی بهم دست داد و خیلی دلم برای داداشم تنگ شد از همون اول. تا حدی که می خواستم برگردم. ساعت ۲:۳۰ فرودگاه بودم. وارد شدم نمیدونستم باید همه وسایلتو بدی از دستگاه رد شه و بعد بری بازرسی بانوان. همینطوری با وسایل رد شدم از دستگاه بخش بانوانو خانومه بیدار شد و برام توضیح داد و خندم گرفت.</p>

<p>محیط اولش خلوت بود ولی زنده بود، چون افراد زیادی اون زمان سر پستشون بودن و مشغول کار. دو سه نفری خوابیده بودن رو صندلیای سالن. منم طبق برنامه قبلیم کتابمو (Red Queen) از کیفم درآوردم شروع کردم خوندن (البته برنامه خوندن Eleven Minutes بود ولی خوب اونو تموم کرده بودم تا اون موقع).</p>

<p>خلاصه ساعت از  ۳:۳۰ رد شده بود و منم دراز کشیدم رو صندلیا و داشتم کتابمو می خوندم بلکه بتونم یکم بخوابم قبل پرواز. ولی سرمای محیط و هیجان خودم مانع خواب شد.</p>

<p>ساعت ۵ اعلام کردن که کارت پرواز عسلویه رو بیایید بگیرید و بار رو تحویل بدید. منم یه مشماع بزرگ برده بودم که کوله‌مو بذارم توشو بعد تحویل بدم، چون می‌خواستم تمیز بمونه. مرحله بعد رفتن به سالن ترانزیته. آخرین باری که سوار هواپیما شدم ۸ سالم بود و چیزی یادم نبود، برای همین همه چی برام تازگی داشت و جالب بود برای همین با جزییات می نویسم :)</p>

<p>۵:۵۵ صف وایسادیم و بعد از تحویل بخشی از کارت پرواز سوار اتوبوسی شدیم که برد سمت هواپیما. هوا هنوز تاریک بود و من پر از هیجان می خواستم همه چیو ببینم. وقتی صندلیمو تو هواپیما پیدا کردم، دیدم جام کنار پنجره است چی از این بهتررررر؟! خیلی خوشحال شدم. همون اول چن تایی عکس انداختم.</p>

<p>اما این شادی و خوشحالی ما دیری نپایید که بوی سیر خوشحالی ما رو از نشستن در کنار پنجره تبدیل به کابوس کرد =_= از قضا آقایی که کنار بنده نشسته بودن سیر میل فرموده بودن و در تمام طول مسیر بنده رو هم از این عطر مستفیض نمودند. رفتار مهماندارا خیلی خوب بود بعد از اینکه همه مسافرا اومدن صفحه نمایش هایی که تو هواپیما بود خوشامد گفت و نحوه استفاده از ماسک و جلیقه رو برای زمان بحران گفت. نکته مهمی که فکر می‌کنم قبلا هم تو پست های قبلی بهش اشاره کردم که اولویت همیشه اول خودتون هستین و بعد بقیه در اینجا این بود که نشون می‌داد اگر زمان استفاده از ماسک رسید باید اول ماسک خودتون رو بزنید و بعد به همراهتون کمک کنید. و در تصویر هم یک مادر و فرزند رو نشون که مادر اول ماسک خودش رو زد و بعد ماسک بچه‌ش.  و بعد هم خلبان سبحانیان خودش و کمک خلبان رو معرفی کرد بعدم مهماندار گفت که هواپیما ایرباس ۳۱۰ سواریم و سرمهماندار منصوری رو معرفی کرد.</p>

<p>ساعت ۶:۵۵ تیک آف بود، حس خوبی داشت و در این حال که من هیجان زده بودم آقای همسفر کناری خوابیده بود. اولش همه خونه‌ها کوچولو شده بودن و بعدش رفتیم بالای ابرا. خیلییییی حس خوبی بود. یه ربع، بیست دقیقه ای از منظره لذت بردم و بعدش دوباره رفتم سراغ رمانم.</p>

<p>وقتی رسیدیم نمی‌دونستم کمربند رو چطوری باز می‌کنن، خلاصه یکم ور رفتم باهاش تا باز شد. دوست داشتم همینطوری می‌موندم تو هواپیما.</p>

<p>وارد سالن فرودگاه عسلویه شدم بعد از چند دقیقه بارا اومد تو سالن. کوله قشنگم کاملا تمیز بود.</p>

<p>کیفمو برداشتم و از در اومدم بیرون یه گرمای خیلی جذابی(به قول یکی که تو سفر باهاش آشنا شدم خورد تو صورتم) از اونجایی که اومدنی تو فرودگاه تهران داشتم یخ می‌زدم رفتم و یه لباس دیگه زیر مانتو پوشیدم ولی همین که از در اومدم بیرون برگشتم داخل و لباس رو درآوردم. راننده‌ تاکسیا می‌پرسیدن کجا می‌خوای بری و منم گفتم و از ۱۲۰ تا  ۷۵ تومن پیشنهاد دادن تا مقصد. ولی من راه افتادم سمت جاده که مردمی که این مسیرو میرن باهاشون برم تا به نوعی هیچ رو تجربه کنم. اما تجربه خوبی نبود.</p>

<p>‌خلاصه با چند تا ماشین بالاخره نزدیکای ظهر رسیدم به مقصد. این ماشینایی که سوار می‌شدم صحبت که میشد اول می‌پرسیدن از کجا و برای چی اومدی اینجا، وقتی می‌گفتم تهران خیلی تعجب می‌کردن و وقتی هم که می‌گفتم برای کار داوطلبانه کلا متوجه منظورم نمی‌شدن. خلاصه به دو تا ماشین اول گفتم از تهران اومدم ولی دیگه از اون به بعد گفتم شیراز :)</p>

<p>وقتی رسیدم اول رفتم کنار دریا. فیلم گرفتم و به دوستانی که جویای احوال بودن فرستادم.</p>

<p>بعد یکی از بومگردی اومد و صدام زد منم رفتم داخل.</p>

<p>شهریور کنار دریای جنوب. وقتی از خونه بیرون می‌اومدم انگار یکی گردنمو گرفته بود فشار می‌داد. حقیقتش خیلی خوب نبود اون تجربه و به همین دلیل یکی دو روز اونجا موندم فقط. سفری که قرار بود حداقل ۵ روزه باشه. اما بعدش حالم خوب بود.</p>

<p>این تجربه باعث شد خیلی با دید آگاهانه‌تری سفرهامو انتخاب کنم. بوم‌گردی‌ای که رفتم رو یه زوج جوان با یه بچه کوچک می‌گردوندن، خانمه خیلی محلم نمی‌ذاشت. مرده می‌گفت یه نفر قبل من برای کار داوطلبانه اومده بوده که رفتار نامناسبی داشته و فکر کردم حتما به همین دلیله که خانمش با من اینطوریه و دیدش کلا عوض شده نسبت به کسایی که برای کار داوطلبی میان، و می‌گفت دیگه هم نمی‌خوان اینطوری نیرو بگیرن.</p>

<p>خلاصه که حساب همه چیو باید بکنید وقتی اینطور سفرها رو انتخاب می‌کنید.</p>

</div>]]>
    </content>
  </entry>

</feed>
