

<article class="post-content">

  <div class="post-header">
    <h1 class="post-title">مبانی پردازش زبان طبیعی(NLP)- دو</h1>
    <p>
      <i class="fas fa-calendar"></i>
      چهارشنبه ۲۳ تیر ۱۴۰۰<br>
      
      <i class="fas fa-stopwatch"></i>
      <p>در قسمت قبل فایل متنی رو به دو روش آسان و دشوار خوندیم. و پاکسازی دیتا رو توسط سه متد پیش پردازش یاد گرفتیم: حذف علائم نگارشی، توکنایز کردن (جداسازی کلمات)، حذف کلمات بدون معنی. و گفتیم که یک مرحله چهارمی هم برای پاکسازی یا نرمالسازی داده متنی می شه استفاده کرد که شاید همیشه به اندازه مراحل قبل اهمیت نداشته باشه. در این قسمت درباره این مرحله چهارم صحبت می کنیم.</p>

<h۲ id="بخش-پنجم-stemming"><strong>بخش پنجم: Stemming</strong></h۲>

<p>کاری که stemming انجام می ده اینه که میاد پسوند و پیشوند رو از کلمه حذف می کنه و هر چی باقی موند رو به عنوان خروجی میده، پس ممکنه خروجی حتی کلمه نباشه. خب پس چرا ازش استفاده می کنیم؟ بخاطر سادگی و سرعتی که داره.</p>

<p>اول از همه روی یک سری کلمات این روش رو اجرا می کنیم تا بیشتر باهاش آشنا شیم و بعد روی دیتاستی که در قسمت قبل بررسی کردیم. <br />
باید اول کتابخونه <code class="language-plaintext highlighter-rouge">nltk</code> رو ایمپورت کنیم. سپس از  استمر <code class="language-plaintext highlighter-rouge">PorterStemmer()</code>  استفاده می کنیم. این استمرها برای هر زبانی متفاوته، برای زبان انگلیسی دو استمر <code class="language-plaintext highlighter-rouge">PorterStammer</code> و <code class="language-plaintext highlighter-rouge">LancasterStammer</code> وجود داره. اولی رایجتره و سریعتر.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import nltk
ps = nltk.PorterStemmer()
</code></pre></div></div>

<p>با استفاده از <code class="language-plaintext highlighter-rouge">dir(ps)</code> توابعی که این استمر داره رو می تونیم مشاهده کنیم. تابع <code class="language-plaintext highlighter-rouge">stem</code> بیشتر استفاده می شه.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>print(ps.stem('grow'))
print(ps.stem('growing'))
print(ps.stem('grows'))
</code></pre></div></div>

<p>همه این کلمات رو خلاصه می کنه به grow. در مثال بعدی فرق فعل و فاعل رو می دونه.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>print(ps.stem('runs'))  
print(ps.stem('running'))
print(ps.stem('runner'))
</code></pre></div></div>

<p>حالا بریم سراغ دیتاست پیام های اسپم و غیر اسپم.<br />
ابتدا دیتا رو می خونیم:
    import pandas as pd
    import re
    import string</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>nltk.download('stopwords')
stopword = nltk.corpus.stopwords.words('english')
pd.set_option('display.max_colwidth', ۱۰۰)

dataset = pd.read_csv('SMSSpamCollection.tsv', sep='\t', header=None)
dataset.columns = ('label', 'body')
dataset.head()
</code></pre></div></div>

<p>کتابخونه های مورد نیاز رو هم من همین ابتدا ایمپورت کردم. سپس سه مرحله پاکسازی دیتا رو انجام  می دیم:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def clean_data(text):
    text = [word for word in text if word not in string.punctuation]
    tokens = re.split('\W+', text)
    text = [word for word in tokens if word not in stopword]
    return text

dataset['tokenized_text'] = dataset['body'].apply(lambda x: clean_data(x))



def stemming(tokenized_text):
    text = [ps.stem(word) for word in tokenized_text]
    return text

dataset['stemmed_text'] = dataset['tokenized_text'].apply(lambda x: stemming(x))
</code></pre></div></div>

<h۲ id="بخش-ششم-lemmatization"><strong>بخش ششم: Lemmatization</strong></h۲>

<p>همونطور که دیدیم خروجی <code class="language-plaintext highlighter-rouge">stemming</code> لزوما کلمه نیست و ممکنه یک چیز بی معنی باشه یا حتی اشتباه. یک متد دیگه <code class="language-plaintext highlighter-rouge">Lemmatization</code> نام داره که خروجی این روش حتما کلمه ای در دیکشنریه. یعنی معمولا کلمات رو می بره به ریشه شون.</p>

<p>به مثال های زیر توجه کنید.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>print(ps.stem('meaning')) ==&gt; mean
print(ps.stem('meanness')) ==&gt; mean

print(wn.lemmatize('meaning')) ==&gt; meaning
print(wn.lemmatize('meanness')) ==&gt; meanness


print(ps.stem('goose')) ==&gt; goos
print(ps.stem('geese')) ==&gt; gees

print(wn.lemmatize('goose')) ==&gt; goose
print(wn.lemmatize('geese')) ==&gt; goose
</code></pre></div></div>

<p>در واقع stemming رویکرد الگوریتمی داره و فقط با رشته ای که بهش می دیم کار می کنه و فقط پسوند رو حذف می کنه.<br />
اما lemmatization پیچیده تره و کلمه ای که بهش داده می شه رو در لیست لغات بررسی می کنه و پرداش می کنه و بعد ریشه کلمه رو بر می گردونه مشکلش اینه که اگر کلمه ای که بهش داده شده در لیست لغات نباشه همونو برمی گردونه.</p>

<p>همین اتفاقی که در این مثال ها افتاده، که همونطور که می بینیم خلاصه نکردن بهتر از رشته کلمه اشتباه برگردوندنه.</p>

<p>حالا می خوایم تکنیک lemmatization رو روی دیتاست پیام ها پیاده کنیم. مثل قبل، ابتدا دیتا رو می خونیم و پاکسازی های اولیه رو انجام می دیم و بعد از lemmatizer استفاده می کنیم. <br />
در اینجا من فقط تابع lemmatizer رو نوشتم. کد کامل رو <a href="https://github.com/spacelover۱/NLP-with-Python/blob/main/۲-SupplementalDataCleaning/UsingaLemmatizer.ipynb">اینجا</a> می تونید مشاهده کنید.</p>

<p>برای درک بهتر این دو روش می تونید <a href="https://www.datacamp.com/community/tutorials/stemming-lemmatization-python">این توضیح انگلیسی</a> رو مطالعه کنید.</p>

 دقیقه مطالعه
    </p>
  </div>

  <p>در قسمت قبل فایل متنی رو به دو روش آسان و دشوار خوندیم. و پاکسازی دیتا رو توسط سه متد پیش پردازش یاد گرفتیم: حذف علائم نگارشی، توکنایز کردن (جداسازی کلمات)، حذف کلمات بدون معنی. و گفتیم که یک مرحله چهارمی هم برای پاکسازی یا نرمالسازی داده متنی می شه استفاده کرد که شاید همیشه به اندازه مراحل قبل اهمیت نداشته باشه. در این قسمت درباره این مرحله چهارم صحبت می کنیم.</p>

<h2 id="بخش-پنجم-stemming"><strong>بخش پنجم: Stemming</strong></h2>

<p>کاری که stemming انجام می ده اینه که میاد پسوند و پیشوند رو از کلمه حذف می کنه و هر چی باقی موند رو به عنوان خروجی میده، پس ممکنه خروجی حتی کلمه نباشه. خب پس چرا ازش استفاده می کنیم؟ بخاطر سادگی و سرعتی که داره.</p>

<p>اول از همه روی یک سری کلمات این روش رو اجرا می کنیم تا بیشتر باهاش آشنا شیم و بعد روی دیتاستی که در قسمت قبل بررسی کردیم. <br />
باید اول کتابخونه <code class="language-plaintext highlighter-rouge">nltk</code> رو ایمپورت کنیم. سپس از  استمر <code class="language-plaintext highlighter-rouge">PorterStemmer()</code>  استفاده می کنیم. این استمرها برای هر زبانی متفاوته، برای زبان انگلیسی دو استمر <code class="language-plaintext highlighter-rouge">PorterStammer</code> و <code class="language-plaintext highlighter-rouge">LancasterStammer</code> وجود داره. اولی رایجتره و سریعتر.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import nltk
ps = nltk.PorterStemmer()
</code></pre></div></div>

<p>با استفاده از <code class="language-plaintext highlighter-rouge">dir(ps)</code> توابعی که این استمر داره رو می تونیم مشاهده کنیم. تابع <code class="language-plaintext highlighter-rouge">stem</code> بیشتر استفاده می شه.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>print(ps.stem('grow'))
print(ps.stem('growing'))
print(ps.stem('grows'))
</code></pre></div></div>

<p>همه این کلمات رو خلاصه می کنه به grow. در مثال بعدی فرق فعل و فاعل رو می دونه.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>print(ps.stem('runs'))  
print(ps.stem('running'))
print(ps.stem('runner'))
</code></pre></div></div>

<p>حالا بریم سراغ دیتاست پیام های اسپم و غیر اسپم.<br />
ابتدا دیتا رو می خونیم:
    import pandas as pd
    import re
    import string</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>nltk.download('stopwords')
stopword = nltk.corpus.stopwords.words('english')
pd.set_option('display.max_colwidth', 100)

dataset = pd.read_csv('SMSSpamCollection.tsv', sep='\t', header=None)
dataset.columns = ('label', 'body')
dataset.head()
</code></pre></div></div>

<p>کتابخونه های مورد نیاز رو هم من همین ابتدا ایمپورت کردم. سپس سه مرحله پاکسازی دیتا رو انجام  می دیم:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def clean_data(text):
    text = [word for word in text if word not in string.punctuation]
    tokens = re.split('\W+', text)
    text = [word for word in tokens if word not in stopword]
    return text

dataset['tokenized_text'] = dataset['body'].apply(lambda x: clean_data(x))



def stemming(tokenized_text):
    text = [ps.stem(word) for word in tokenized_text]
    return text

dataset['stemmed_text'] = dataset['tokenized_text'].apply(lambda x: stemming(x))
</code></pre></div></div>

<h2 id="بخش-ششم-lemmatization"><strong>بخش ششم: Lemmatization</strong></h2>

<p>همونطور که دیدیم خروجی <code class="language-plaintext highlighter-rouge">stemming</code> لزوما کلمه نیست و ممکنه یک چیز بی معنی باشه یا حتی اشتباه. یک متد دیگه <code class="language-plaintext highlighter-rouge">Lemmatization</code> نام داره که خروجی این روش حتما کلمه ای در دیکشنریه. یعنی معمولا کلمات رو می بره به ریشه شون.</p>

<p>به مثال های زیر توجه کنید.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>print(ps.stem('meaning')) ==&gt; mean
print(ps.stem('meanness')) ==&gt; mean

print(wn.lemmatize('meaning')) ==&gt; meaning
print(wn.lemmatize('meanness')) ==&gt; meanness


print(ps.stem('goose')) ==&gt; goos
print(ps.stem('geese')) ==&gt; gees

print(wn.lemmatize('goose')) ==&gt; goose
print(wn.lemmatize('geese')) ==&gt; goose
</code></pre></div></div>

<p>در واقع stemming رویکرد الگوریتمی داره و فقط با رشته ای که بهش می دیم کار می کنه و فقط پسوند رو حذف می کنه.<br />
اما lemmatization پیچیده تره و کلمه ای که بهش داده می شه رو در لیست لغات بررسی می کنه و پرداش می کنه و بعد ریشه کلمه رو بر می گردونه مشکلش اینه که اگر کلمه ای که بهش داده شده در لیست لغات نباشه همونو برمی گردونه.</p>

<p>همین اتفاقی که در این مثال ها افتاده، که همونطور که می بینیم خلاصه نکردن بهتر از رشته کلمه اشتباه برگردوندنه.</p>

<p>حالا می خوایم تکنیک lemmatization رو روی دیتاست پیام ها پیاده کنیم. مثل قبل، ابتدا دیتا رو می خونیم و پاکسازی های اولیه رو انجام می دیم و بعد از lemmatizer استفاده می کنیم. <br />
در اینجا من فقط تابع lemmatizer رو نوشتم. کد کامل رو <a href="https://github.com/spacelover1/NLP-with-Python/blob/main/2-SupplementalDataCleaning/UsingaLemmatizer.ipynb">اینجا</a> می تونید مشاهده کنید.</p>

<p>برای درک بهتر این دو روش می تونید <a href="https://www.datacamp.com/community/tutorials/stemming-lemmatization-python">این توضیح انگلیسی</a> رو مطالعه کنید.</p>



  
    <div id="related-posts">
      <h3>مطالب مرتبط</h3>
      <ul>
        
          <li><a href="/debug.html">test</a></li>
        
          <li><a href="/unresolved.html">مسئله حل نشده</a></li>
        
          <li><a href="/physics.html">فیزیک</a></li>
        
          <li><a href="/history.html">بماند به یادگار</a></li>
        
          <li><a href="/red-flags-run.html">اگر اینا رو دیدی فرار کن</a></li>
        
        </ul>
    </div>
  

  <small id="post-tags">
    
      <i class="fas fa-tag"></i>
      <a rel="tag" href="nlp">nlp</a>
    
      <i class="fas fa-tag"></i>
      <a rel="tag" href="اموزش">اموزش</a>
    

    <i class="fas fa-code"></i>
    
    
    <a href="https://raw.githubusercontent.com/spacelover1/personalBlog/master/_posts/2021-07-14-nlp-basics-two.md">سورس</a>
    
  </small>

  <nav class="pagination">
    
      <a href="/personalBlog/nlp-basics-three.html" class="pagination--pager" title="مبانی پردازش زبان طبیعی(NLP)- سه">قبلی</a>
    

    
      <a href="/personalBlog/nlp-basics-one.html" class="pagination--pager" title="مبانی پردازش زبان طبیعی(NLP)- یک">بعدی </a>
    
  </nav>

  <section>
    
  </section>

</article>
